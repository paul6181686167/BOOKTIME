#!/usr/bin/env python3
"""
ğŸ”§ INTÃ‰GRATION SÃ‰RIES ULTRA HARVEST EN ATTENTE
Script pour identifier et intÃ©grer les sÃ©ries dÃ©tectÃ©es mais pas encore ajoutÃ©es
"""

import json
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
import hashlib

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_current_series():
    """Charger la base actuelle des sÃ©ries"""
    series_db_path = Path('/app/backend/data/extended_series_database.json')
    
    try:
        with open(series_db_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
            return {series['name'].lower().strip() for series in data}
    except Exception as e:
        logger.error(f"âŒ Erreur lecture base sÃ©ries: {e}")
        return set()

def load_ultra_harvest_backups():
    """Charger toutes les donnÃ©es des backups Ultra Harvest rÃ©cents"""
    backup_dir = Path('/app/backups/series_detection')
    ultra_harvest_files = [
        'backup_ultra_harvest_20250712_100905.json',
        'backup_ultra_harvest_20250712_102352.json',
        'backup_20250712_013649_mega_harvest.json',
        'backup_20250712_114252_mega_expansion.json',
        'backup_20250712_113447_mega_expansion.json',
        'backup_20250712_114505_ultra_expansion.json',
        'backup_20250712_113650_ultra_expansion.json'
    ]
    
    all_series = []
    
    for filename in ultra_harvest_files:
        file_path = backup_dir / filename
        if file_path.exists():
            try:
                logger.info(f"ğŸ“ Traitement {filename}...")
                with open(file_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list):
                        # Filtrer les sÃ©ries valides seulement
                        valid_series = [s for s in data if 'name' in s and len(s.get('name', '').strip()) >= 3]
                        all_series.extend(valid_series)
                        logger.info(f"ğŸ“ {filename}: {len(valid_series)} sÃ©ries valides chargÃ©es")
                    else:
                        logger.warning(f"âš ï¸ Format inattendu dans {filename}")
            except Exception as e:
                logger.error(f"âŒ Erreur lecture {filename}: {e}")
    
    return all_series

def identify_new_series(all_harvest_series, current_series):
    """Identifier les nouvelles sÃ©ries non encore intÃ©grÃ©es"""
    new_series = []
    duplicates = 0
    
    for series in all_harvest_series:
        if 'name' in series:
            series_name = series['name'].lower().strip()
            
            # Filtrer les sÃ©ries dÃ©jÃ  prÃ©sentes
            if series_name not in current_series:
                # VÃ©rifications qualitÃ© basiques
                if (len(series_name) >= 3 and 
                    series_name not in ['', 'unknown', 'n/a'] and
                    not series_name.startswith('error')):
                    new_series.append(series)
                    current_series.add(series_name)  # Ã‰viter doublons internes
                else:
                    logger.debug(f"ğŸš« SÃ©rie rejetÃ©e (qualitÃ©): {series_name}")
            else:
                duplicates += 1
    
    logger.info(f"ğŸ†• {len(new_series)} nouvelles sÃ©ries identifiÃ©es")
    logger.info(f"ğŸ”„ {duplicates} doublons Ã©vitÃ©s")
    
    return new_series

def enhance_series_metadata(new_series):
    """AmÃ©liorer les mÃ©tadonnÃ©es des nouvelles sÃ©ries"""
    enhanced_series = []
    
    for series in new_series:
        enhanced = series.copy()
        
        # Valeurs par dÃ©faut si manquantes
        if 'category' not in enhanced:
            enhanced['category'] = 'roman'  # DÃ©faut
        
        if 'volumes' not in enhanced:
            enhanced['volumes'] = 1
            
        if 'confidence' not in enhanced:
            enhanced['confidence'] = 80
            
        if 'source' not in enhanced:
            enhanced['source'] = 'ultra_harvest_pending'
            
        # Timestamp ajout
        enhanced['date_added'] = datetime.now().isoformat()
        
        # GÃ©nÃ©ration ID unique
        series_id = hashlib.md5(
            f"{enhanced['name']}_{enhanced.get('category', 'roman')}".encode()
        ).hexdigest()[:12]
        enhanced['id'] = series_id
        
        enhanced_series.append(enhanced)
    
    return enhanced_series

def save_expanded_database(current_data, new_series):
    """Sauvegarder la base Ã©tendue avec nouvelles sÃ©ries"""
    
    # Backup de sÃ©curitÃ©
    backup_path = f"/app/backups/series_detection/backup_before_pending_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(backup_path, 'w', encoding='utf-8') as f:
            json.dump(current_data, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ’¾ Backup sauvegardÃ©: {backup_path}")
    except Exception as e:
        logger.error(f"âŒ Erreur backup: {e}")
        return False
    
    # Extension base avec nouvelles sÃ©ries
    extended_database = current_data + new_series
    
    # Sauvegarde base Ã©tendue
    db_path = Path('/app/backend/data/extended_series_database.json')
    try:
        with open(db_path, 'w', encoding='utf-8') as f:
            json.dump(extended_database, f, ensure_ascii=False, indent=2)
        
        logger.info(f"âœ… Base Ã©tendue sauvegardÃ©e: {len(extended_database)} sÃ©ries totales")
        return True
        
    except Exception as e:
        logger.error(f"âŒ Erreur sauvegarde: {e}")
        return False

def generate_integration_report(new_series):
    """GÃ©nÃ©rer rapport d'intÃ©gration"""
    report = {
        'timestamp': datetime.now().isoformat(),
        'operation': 'pending_series_integration',
        'new_series_added': len(new_series),
        'categories_breakdown': defaultdict(int),
        'sample_series': new_series[:20] if new_series else []
    }
    
    # Analyse par catÃ©gorie
    for series in new_series:
        category = series.get('category', 'unknown')
        report['categories_breakdown'][category] += 1
    
    # Sauvegarde rapport
    report_path = f"/app/reports/pending_integration_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    try:
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“Š Rapport gÃ©nÃ©rÃ©: {report_path}")
    except Exception as e:
        logger.error(f"âŒ Erreur rapport: {e}")
    
    return report

def main():
    """Processus principal d'intÃ©gration"""
    logger.info("ğŸš€ DÃ‰MARRAGE INTÃ‰GRATION SÃ‰RIES EN ATTENTE")
    
    # 1. Charger base actuelle
    logger.info("ğŸ“š Chargement base sÃ©ries actuelle...")
    current_series_names = load_current_series()
    logger.info(f"ğŸ“Š {len(current_series_names)} sÃ©ries actuellement en base")
    
    # Charger donnÃ©es complÃ¨tes pour extension
    try:
        with open('/app/backend/data/extended_series_database.json', 'r', encoding='utf-8') as f:
            current_data = json.load(f)
    except Exception as e:
        logger.error(f"âŒ Erreur chargement donnÃ©es complÃ¨tes: {e}")
        return False
    
    # 2. Charger donnÃ©es Ultra Harvest
    logger.info("ğŸ” Chargement donnÃ©es Ultra Harvest...")
    all_harvest_series = load_ultra_harvest_backups()
    logger.info(f"ğŸ“Š {len(all_harvest_series)} sÃ©ries dans backups Ultra Harvest")
    
    # 3. Identifier nouvelles sÃ©ries
    logger.info("ğŸ†• Identification nouvelles sÃ©ries...")
    new_series = identify_new_series(all_harvest_series, current_series_names)
    
    if not new_series:
        logger.info("âœ… Aucune nouvelle sÃ©rie Ã  intÃ©grer - Base Ã  jour")
        return True
    
    # 4. AmÃ©liorer mÃ©tadonnÃ©es
    logger.info("ğŸ”§ AmÃ©lioration mÃ©tadonnÃ©es...")
    enhanced_series = enhance_series_metadata(new_series)
    
    # 5. Sauvegarder base Ã©tendue
    logger.info("ğŸ’¾ Sauvegarde base Ã©tendue...")
    if save_expanded_database(current_data, enhanced_series):
        # 6. GÃ©nÃ©rer rapport
        logger.info("ğŸ“Š GÃ©nÃ©ration rapport...")
        report = generate_integration_report(enhanced_series)
        
        logger.info("="*60)
        logger.info("ğŸ¯ INTÃ‰GRATION RÃ‰USSIE !")
        logger.info(f"ğŸ“ˆ {len(enhanced_series)} nouvelles sÃ©ries ajoutÃ©es")
        logger.info(f"ğŸ“š {len(current_data) + len(enhanced_series)} sÃ©ries totales en base")
        logger.info("="*60)
        
        return True
    else:
        logger.error("âŒ Ã‰chec sauvegarde")
        return False

if __name__ == "__main__":
    success = main()
    exit(0 if success else 1)