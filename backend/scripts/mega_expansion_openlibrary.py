#!/usr/bin/env python3
"""
ğŸš€ MEGA EXPANSION OPEN LIBRARY - AUTOEXPANSION MAXIMALE
Script d'expansion massive pour ajouter le maximum de sÃ©ries possibles

StratÃ©gies multiples :
1. Recherche par mots-clÃ©s sÃ©rie populaires
2. Exploration auteurs prolifiques internationaux
3. Scan catÃ©gories avec patterns sÃ©rie
4. Recherche sÃ©ries par volume numbers
5. Exploration franchises populaires
"""

import asyncio
import aiohttp
import json
import re
from typing import List, Dict, Optional, Set
from datetime import datetime
import logging
from pathlib import Path
import random
import time

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/mega_expansion.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class MegaExpansionOpenLibrary:
    """Expansion massive sÃ©ries Open Library avec stratÃ©gies multiples"""
    
    def __init__(self):
        self.base_url = "https://openlibrary.org"
        self.session = None
        self.existing_series = set()
        self.new_series = []
        self.stats = {
            'queries_made': 0,
            'books_analyzed': 0,
            'series_detected': 0,
            'new_series_added': 0,
            'duplicates_skipped': 0,
            'processing_time': 0
        }
        
        # StratÃ©gies d'expansion maximale
        self.series_keywords = [
            # Patterns sÃ©rie universels
            "Book 1", "Volume 1", "Part 1", "Episode 1", "Tome 1",
            "Series", "Saga", "Chronicles", "Adventures", "Tales",
            "Cycle", "Collection", "Universe", "World", "Legacy",
            
            # Termes sÃ©rie populaires
            "Trilogy", "Quartet", "Quintet", "Hexalogy", "Heptalogy",
            "Octology", "Ennealogy", "Decalogy", "Duology",
            
            # Genres avec sÃ©ries
            "Fantasy Series", "Mystery Series", "Crime Series",
            "Science Fiction Series", "Romance Series", "Horror Series",
            "Thriller Series", "Adventure Series", "Historical Series",
            
            # Patterns manga/comics
            "Manga Series", "Comic Series", "Graphic Novel Series",
            "Light Novel", "Web Novel", "Visual Novel"
        ]
        
        # Auteurs mega-prolifiques (50+ livres)
        self.prolific_authors = [
            # Super auteurs anglais/amÃ©ricains
            "Isaac Asimov", "Stephen King", "Agatha Christie", "Louis L'Amour",
            "Ray Bradbury", "Philip K. Dick", "Ursula K. Le Guin", "Arthur C. Clarke",
            "Robert A. Heinlein", "Andre Norton", "Piers Anthony", "Terry Pratchett",
            "Mercedes Lackey", "David Weber", "John Ringo", "Eric Flint",
            "Baen Books", "Tor Books", "DAW Books", "Ace Books",
            
            # Auteurs franÃ§ais prolifiques
            "Jules Verne", "Alexandre Dumas", "HonorÃ© de Balzac", "Ã‰mile Zola",
            "Victor Hugo", "Guy de Maupassant", "Marcel Proust", "AndrÃ© Gide",
            "Jean-Paul Sartre", "Albert Camus", "Simone de Beauvoir",
            "Michel Houellebecq", "AmÃ©lie Nothomb", "Guillaume Musso",
            "Marc Levy", "Anna Gavalda", "Katherine Pancol",
            
            # Mangaka populaires
            "Osamu Tezuka", "Akira Toriyama", "Eiichiro Oda", "Masashi Kishimoto",
            "Tite Kubo", "Hajime Isayama", "Naoki Urasawa", "Kentaro Miura",
            "Hiromu Arakawa", "Takeshi Obata", "Tsugumi Ohba", "ONE",
            "Koyoharu Gotouge", "Gege Akutami", "Tatsuki Fujimoto",
            
            # Auteurs comics occidentaux
            "Stan Lee", "Jack Kirby", "Bob Kane", "Jerry Siegel", "Joe Shuster",
            "Frank Miller", "Alan Moore", "Neil Gaiman", "Grant Morrison",
            "Warren Ellis", "Brian K. Vaughan", "Mark Millar", "Garth Ennis"
        ]
        
        # Franchises et univers populaires
        self.popular_franchises = [
            # Univers fantasy/SF
            "Star Wars", "Star Trek", "Doctor Who", "Warhammer", "Dungeons Dragons",
            "Marvel", "DC Comics", "Transformers", "G.I. Joe", "He-Man",
            "Teenage Mutant Ninja Turtles", "Power Rangers", "Pokemon",
            
            # Univers littÃ©raires
            "Sherlock Holmes", "James Bond", "Jack Ryan", "Jason Bourne",
            "John Carter", "Tarzan", "Zorro", "The Shadow", "Doc Savage",
            "Conan", "Elric", "Fafhrd and Gray Mouser",
            
            # Univers manga/anime
            "Dragon Ball", "Naruto", "One Piece", "Bleach", "Death Note",
            "Attack on Titan", "My Hero Academia", "Demon Slayer", "Jujutsu Kaisen",
            "Hunter x Hunter", "Full Metal Alchemist", "Cowboy Bebop",
            
            # Univers BD franco-belge
            "Asterix", "Tintin", "Lucky Luke", "Spirou", "Gaston Lagaffe",
            "Blake et Mortimer", "Yoko Tsuno", "Thorgal", "Largo Winch",
            "Corto Maltese", "Blacksad", "Lanfeust", "Les Legendaires"
        ]
        
        # Categories Ã©tendues avec sous-genres
        self.mega_categories = {
            'fantasy': [
                'epic fantasy', 'urban fantasy', 'dark fantasy', 'high fantasy',
                'sword and sorcery', 'heroic fantasy', 'portal fantasy',
                'fairy tale retelling', 'mythic fantasy', 'steampunk fantasy'
            ],
            'scifi': [
                'space opera', 'cyberpunk', 'hard science fiction', 'soft science fiction',
                'dystopian', 'alternate history', 'time travel', 'alien invasion',
                'robot fiction', 'steampunk', 'biopunk', 'post-apocalyptic'
            ],
            'mystery': [
                'cozy mystery', 'hard-boiled', 'police procedural', 'detective fiction',
                'noir', 'whodunit', 'locked room mystery', 'amateur sleuth',
                'psychological thriller', 'legal thriller', 'espionage'
            ],
            'romance': [
                'contemporary romance', 'historical romance', 'paranormal romance',
                'romantic suspense', 'erotic romance', 'young adult romance',
                'romantic comedy', 'category romance', 'regency romance'
            ],
            'horror': [
                'supernatural horror', 'psychological horror', 'gothic horror',
                'cosmic horror', 'body horror', 'zombie fiction', 'vampire fiction',
                'werewolf fiction', 'ghost stories', 'haunted house'
            ],
            'manga': [
                'shonen manga', 'shojo manga', 'seinen manga', 'josei manga',
                'kodomomuke manga', 'doujinshi', 'manhwa', 'manhua',
                'light novel', 'visual novel adaptation'
            ],
            'comics': [
                'superhero comics', 'indie comics', 'graphic novels', 'webcomics',
                'european comics', 'franco-belgian comics', 'manga-influenced comics',
                'slice of life comics', 'adventure comics', 'crime comics'
            ]
        }
    
    async def __aenter__(self):
        """Initialisation session async"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=45),
            headers={'User-Agent': 'BOOKTIME-MegaExpansion/2.0'}
        )
        await self.load_existing_series()
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Fermeture session"""
        if self.session:
            await self.session.close()
    
    async def load_existing_series(self):
        """Charge les sÃ©ries existantes pour Ã©viter doublons"""
        try:
            with open('/app/backend/data/extended_series_database.json', 'r') as f:
                existing_data = json.load(f)
                self.existing_series = {series['name'].lower() for series in existing_data}
            logger.info(f"ğŸ“š ChargÃ© {len(self.existing_series)} sÃ©ries existantes")
        except Exception as e:
            logger.warning(f"âš ï¸ Erreur chargement sÃ©ries existantes: {e}")
            self.existing_series = set()
    
    async def search_open_library(self, query: str, limit: int = 1000) -> List[Dict]:
        """Recherche dans Open Library avec gestion d'erreur robuste"""
        try:
            # DÃ©lai anti-rate-limiting
            await asyncio.sleep(random.uniform(0.1, 0.3))
            
            url = f"{self.base_url}/search.json"
            params = {
                'q': query,
                'limit': limit,
                'fields': 'key,title,author_name,subject,first_publish_year,number_of_pages_median,cover_i'
            }
            
            async with self.session.get(url, params=params) as response:
                if response.status == 200:
                    data = await response.json()
                    self.stats['queries_made'] += 1
                    books = data.get('docs', [])
                    self.stats['books_analyzed'] += len(books)
                    logger.info(f"ğŸ” Query '{query}': {len(books)} livres trouvÃ©s")
                    return books
                else:
                    logger.warning(f"âš ï¸ Erreur API {response.status} pour query: {query}")
                    return []
        except Exception as e:
            logger.error(f"âŒ Erreur recherche '{query}': {e}")
            return []
    
    def detect_series_patterns(self, books: List[Dict]) -> List[Dict]:
        """DÃ©tection intelligente de patterns de sÃ©ries"""
        series_candidates = {}
        
        for book in books:
            title = book.get('title', '').strip()
            authors = book.get('author_name', [])
            
            if not title or not authors:
                continue
            
            # Patterns de dÃ©tection sÃ©rie
            series_patterns = [
                # NumÃ©rotation explicite
                r'(.+?)\s+(?:Book|Volume|Part|Tome|Episode)\s+(\d+)',
                r'(.+?)\s+(\d+)(?:st|nd|rd|th)?(?:\s|$)',
                r'(.+?)\s+#(\d+)',
                r'(.+?):\s*(.+)',  # Titre: Sous-titre
                
                # Patterns sÃ©rie
                r'(.+?)\s+(?:Series|Saga|Chronicles|Adventures|Tales)',
                r'(.+?)\s+(?:Cycle|Collection|Universe|World)',
                
                # Patterns spÃ©ciaux
                r'The\s+(.+?)\s+(?:Book|Volume|Part)',
                r'(.+?)\s+Trilogy',
                r'(.+?)\s+Quartet',
            ]
            
            for pattern in series_patterns:
                match = re.search(pattern, title, re.IGNORECASE)
                if match:
                    series_name = match.group(1).strip()
                    if len(series_name) > 3:  # Nom sÃ©rie valide
                        series_key = (series_name.lower(), authors[0] if authors else "Unknown")
                        
                        if series_key not in series_candidates:
                            series_candidates[series_key] = {
                                'name': series_name,
                                'author': authors[0] if authors else "Unknown",
                                'books': [],
                                'volumes': set(),
                                'subjects': set()
                            }
                        
                        series_candidates[series_key]['books'].append(book)
                        if match.group(2).isdigit() if len(match.groups()) > 1 else False:
                            series_candidates[series_key]['volumes'].add(int(match.group(2)))
                        
                        # Collecte sujets pour catÃ©gorisation
                        subjects = book.get('subject', [])
                        if subjects:
                            series_candidates[series_key]['subjects'].update(subjects[:5])
                        
                        break
        
        # Filtre sÃ©ries valides (minimum 2 livres ou volumes)
        valid_series = []
        for (series_name, author), data in series_candidates.items():
            if len(data['books']) >= 2 or len(data['volumes']) >= 2:
                if series_name.lower() not in self.existing_series:
                    valid_series.append(data)
                    self.stats['series_detected'] += 1
                else:
                    self.stats['duplicates_skipped'] += 1
        
        return valid_series
    
    def categorize_series(self, series_data: Dict) -> str:
        """CatÃ©gorisation intelligente sÃ©rie basÃ©e sur sujets"""
        subjects = [s.lower() for s in series_data['subjects']]
        
        # Patterns manga/comics
        manga_patterns = ['manga', 'comic', 'graphic novel', 'anime', 'japanese']
        if any(pattern in ' '.join(subjects) for pattern in manga_patterns):
            return 'manga'
        
        # Patterns BD
        bd_patterns = ['bande dessinÃ©e', 'comic book', 'cartoon', 'illustration']
        if any(pattern in ' '.join(subjects) for pattern in bd_patterns):
            return 'bd'
        
        # Par dÃ©faut roman
        return 'roman'
    
    def create_series_entry(self, series_data: Dict) -> Dict:
        """CrÃ©ation entrÃ©e sÃ©rie au format EXTENDED_SERIES_DATABASE"""
        name = series_data['name']
        author = series_data['author']
        category = self.categorize_series(series_data)
        volumes = max(series_data['volumes']) if series_data['volumes'] else len(series_data['books'])
        
        # GÃ©nÃ©ration patterns de dÃ©tection
        base_name = name.lower()
        keywords = [
            base_name,
            f"{base_name} series",
            f"{base_name} saga",
            f"the {base_name}",
            author.lower().split()[0] if author != "Unknown" else ""
        ]
        keywords = [k for k in keywords if k]  # Supprime vides
        
        # Variations titre
        variations = [name, f"The {name}", f"{name} Series"]
        
        # Exclusions pour Ã©viter faux positifs
        exclusions = ["anthology", "collection", "best of", "complete works"]
        
        return {
            "name": name,
            "authors": [author] if author != "Unknown" else [],
            "category": category,
            "volumes": volumes,
            "keywords": keywords,
            "variations": variations,
            "exclusions": exclusions,
            "source": "open_library_mega_expansion",
            "confidence_score": 85,
            "auto_generated": True,
            "detection_date": datetime.now().isoformat(),
            "subjects": list(series_data['subjects'])[:10]
        }
    
    async def expansion_strategy_keywords(self, limit_per_keyword: int = 200) -> List[Dict]:
        """StratÃ©gie 1: Recherche par mots-clÃ©s sÃ©rie"""
        logger.info("ğŸ¯ StratÃ©gie 1: Expansion par mots-clÃ©s sÃ©rie")
        all_series = []
        
        for keyword in self.series_keywords[:50]:  # Limite pour performance
            books = await self.search_open_library(f'"{keyword}"', limit_per_keyword)
            series = self.detect_series_patterns(books)
            all_series.extend(series)
            
            # Log progression
            if len(all_series) % 10 == 0:
                logger.info(f"ğŸ“ˆ Progression: {len(all_series)} sÃ©ries dÃ©tectÃ©es")
        
        logger.info(f"âœ… StratÃ©gie 1 terminÃ©e: {len(all_series)} sÃ©ries trouvÃ©es")
        return all_series
    
    async def expansion_strategy_authors(self, limit_per_author: int = 300) -> List[Dict]:
        """StratÃ©gie 2: Exploration auteurs prolifiques"""
        logger.info("ğŸ‘¥ StratÃ©gie 2: Expansion par auteurs prolifiques")
        all_series = []
        
        for author in self.prolific_authors[:40]:  # Limite pour performance
            books = await self.search_open_library(f'author:"{author}"', limit_per_author)
            series = self.detect_series_patterns(books)
            all_series.extend(series)
            
            # Log progression
            if len(all_series) % 15 == 0:
                logger.info(f"ğŸ“ˆ Progression auteurs: {len(all_series)} sÃ©ries dÃ©tectÃ©es")
        
        logger.info(f"âœ… StratÃ©gie 2 terminÃ©e: {len(all_series)} sÃ©ries trouvÃ©es")
        return all_series
    
    async def expansion_strategy_franchises(self, limit_per_franchise: int = 25) -> List[Dict]:
        """StratÃ©gie 3: Exploration franchises populaires"""
        logger.info("ğŸ¢ StratÃ©gie 3: Expansion par franchises populaires")
        all_series = []
        
        for franchise in self.popular_franchises[:30]:  # Limite pour performance
            books = await self.search_open_library(f'"{franchise}"', limit_per_franchise)
            series = self.detect_series_patterns(books)
            all_series.extend(series)
            
            # Log progression
            if len(all_series) % 10 == 0:
                logger.info(f"ğŸ“ˆ Progression franchises: {len(all_series)} sÃ©ries dÃ©tectÃ©es")
        
        logger.info(f"âœ… StratÃ©gie 3 terminÃ©e: {len(all_series)} sÃ©ries trouvÃ©es")
        return all_series
    
    async def expansion_strategy_categories(self, limit_per_category: int = 40) -> List[Dict]:
        """StratÃ©gie 4: Scan catÃ©gories avec sous-genres"""
        logger.info("ğŸ“š StratÃ©gie 4: Expansion par catÃ©gories Ã©tendues")
        all_series = []
        
        for main_cat, sub_cats in self.mega_categories.items():
            for sub_cat in sub_cats[:8]:  # Limite sous-catÃ©gories
                query = f'subject:"{sub_cat}" AND (series OR saga OR book OR volume)'
                books = await self.search_open_library(query, limit_per_category)
                series = self.detect_series_patterns(books)
                all_series.extend(series)
        
        logger.info(f"âœ… StratÃ©gie 4 terminÃ©e: {len(all_series)} sÃ©ries trouvÃ©es")
        return all_series
    
    def deduplicate_series(self, all_series: List[Dict]) -> List[Dict]:
        """DÃ©duplication intelligente des sÃ©ries trouvÃ©es"""
        seen = set()
        unique_series = []
        
        for series in all_series:
            # ClÃ© de dÃ©duplication basÃ©e sur nom + auteur
            key = (series['name'].lower().strip(), series['author'].lower().strip())
            if key not in seen and series['name'].lower() not in self.existing_series:
                seen.add(key)
                unique_series.append(series)
            else:
                self.stats['duplicates_skipped'] += 1
        
        return unique_series
    
    async def run_mega_expansion(self, max_series: int = 100) -> Dict:
        """ExÃ©cution complÃ¨te mÃ©ga expansion avec toutes stratÃ©gies"""
        start_time = datetime.now()
        logger.info(f"ğŸš€ DÃ‰BUT MÃ‰GA EXPANSION - Objectif: {max_series} nouvelles sÃ©ries")
        
        # ExÃ©cution stratÃ©gies en parallÃ¨le pour performance maximale
        try:
            # StratÃ©gie 1: Mots-clÃ©s sÃ©rie
            series_keywords = await self.expansion_strategy_keywords(20)
            
            # StratÃ©gie 2: Auteurs prolifiques  
            series_authors = await self.expansion_strategy_authors(25)
            
            # StratÃ©gie 3: Franchises populaires
            series_franchises = await self.expansion_strategy_franchises(20)
            
            # StratÃ©gie 4: CatÃ©gories Ã©tendues
            series_categories = await self.expansion_strategy_categories(30)
            
            # Consolidation toutes sÃ©ries
            all_series = series_keywords + series_authors + series_franchises + series_categories
            logger.info(f"ğŸ“Š Total sÃ©ries brutes trouvÃ©es: {len(all_series)}")
            
            # DÃ©duplication
            unique_series = self.deduplicate_series(all_series)
            logger.info(f"ğŸ¯ SÃ©ries uniques aprÃ¨s dÃ©duplication: {len(unique_series)}")
            
            # Limitation au maximum demandÃ©
            final_series = unique_series[:max_series]
            
            # Conversion au format final
            formatted_series = []
            for series in final_series:
                entry = self.create_series_entry(series)
                formatted_series.append(entry)
                self.stats['new_series_added'] += 1
            
            # Sauvegarde nouvelles sÃ©ries
            await self.save_new_series(formatted_series)
            
            # Calcul temps total
            self.stats['processing_time'] = (datetime.now() - start_time).total_seconds()
            
            logger.info(f"âœ… MÃ‰GA EXPANSION TERMINÃ‰E: {len(formatted_series)} nouvelles sÃ©ries ajoutÃ©es")
            return {
                'new_series': formatted_series,
                'stats': self.stats,
                'success': True
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur lors mÃ©ga expansion: {e}")
            return {
                'new_series': [],
                'stats': self.stats,
                'success': False,
                'error': str(e)
            }
    
    async def save_new_series(self, new_series: List[Dict]):
        """Sauvegarde nouvelles sÃ©ries dans database"""
        try:
            # Chargement base existante
            database_path = Path('/app/backend/data/extended_series_database.json')
            with open(database_path, 'r') as f:
                existing_data = json.load(f)
            
            # Ajout nouvelles sÃ©ries
            existing_data.extend(new_series)
            
            # Sauvegarde avec backup
            backup_path = Path(f'/app/backups/series_detection/backup_{datetime.now().strftime("%Y%m%d_%H%M%S")}_mega_expansion.json')
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(backup_path, 'w') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            
            # Sauvegarde principale
            with open(database_path, 'w') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
            
            logger.info(f"ğŸ’¾ Base de donnÃ©es mise Ã  jour: {len(existing_data)} sÃ©ries totales")
            
        except Exception as e:
            logger.error(f"âŒ Erreur sauvegarde: {e}")
            raise

async def main():
    """Point d'entrÃ©e principal"""
    import argparse
    
    parser = argparse.ArgumentParser(description="MÃ©ga Expansion Open Library")
    parser.add_argument('--limit', type=int, default=5000, help='Nombre maximum de nouvelles sÃ©ries (SANS LIMITE)')
    parser.add_argument('--unlimited', action='store_true', help='Mode illimitÃ© (ignorer toutes limites)')
    parser.add_argument('--test', action='store_true', help='Mode test sans sauvegarde')
    args = parser.parse_args()
    
    print(f"""
ğŸš€ MÃ‰GA EXPANSION OPEN LIBRARY
===============================
Objectif: {args.limit} nouvelles sÃ©ries
Mode: {'TEST' if args.test else 'PRODUCTION'}
===============================
""")
    
    async with MegaExpansionOpenLibrary() as expander:
        result = await expander.run_mega_expansion(args.limit)
        
        if result['success']:
            stats = result['stats']
            print(f"""
âœ… MÃ‰GA EXPANSION RÃ‰USSIE !
============================
ğŸ“Š Nouvelles sÃ©ries ajoutÃ©es: {stats['new_series_added']}
ğŸ” RequÃªtes effectuÃ©es: {stats['queries_made']}
ğŸ“š Livres analysÃ©s: {stats['books_analyzed']}
ğŸ¯ SÃ©ries dÃ©tectÃ©es: {stats['series_detected']}
ğŸ”„ Doublons ignorÃ©s: {stats['duplicates_skipped']}
â±ï¸ Temps traitement: {stats['processing_time']:.1f}s
============================
""")
        else:
            print(f"âŒ Ã‰chec mÃ©ga expansion: {result.get('error', 'Erreur inconnue')}")

if __name__ == "__main__":
    asyncio.run(main())