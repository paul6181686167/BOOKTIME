#!/usr/bin/env python3
"""
üîß INT√âGRATION ULTRA HARVEST - CRIT√àRES AJUST√âS POUR MAXIMUM R√âCUP√âRATION
Script optimis√© pour capturer le maximum de nouvelles s√©ries avec crit√®res assouplis
"""

import sqlite3
import json
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Set
import re

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_tracking_data_enhanced():
    """Charger donn√©es avec analyse d√©taill√©e"""
    db_path = Path('/app/data/ultra_harvest_tracking.db')
    
    try:
        with sqlite3.connect(db_path) as conn:
            # R√©cup√©rer TOUS les livres avec s√©ries d√©tect√©es (crit√®res assouplis)
            cursor = conn.execute("""
                SELECT open_library_key, title, author, series_name, confidence_score, 
                       processing_time_ms, source_strategy, isbn, publication_year
                FROM analyzed_books 
                WHERE series_detected = 1 AND series_name IS NOT NULL
                AND series_name != '' AND length(series_name) >= 3
                ORDER BY series_name, title
            """)
            
            books_data = cursor.fetchall()
            logger.info(f"üìö {len(books_data)} livres avec s√©ries d√©tect√©es r√©cup√©r√©s (crit√®res assouplis)")
            
            return books_data
            
    except Exception as e:
        logger.error(f"‚ùå Erreur lecture base tracking: {e}")
        return []

def group_books_by_series_enhanced(books_data):
    """Grouper livres par s√©rie avec crit√®res assouplis"""
    series_candidates = defaultdict(lambda: {
        'books': [],
        'authors': set(),
        'confidence_scores': [],
        'detection_patterns': set(),
        'publication_years': [],
        'isbns': []
    })
    
    for book in books_data:
        (ol_key, title, author, series_name, confidence, processing_time, 
         strategy, isbn, pub_year) = book
        
        # Nettoyage nom s√©rie
        clean_series_name = series_name.strip()
        clean_author = author.strip() if author else "Unknown"
        
        # Cl√© unique pour la s√©rie
        series_key = (clean_series_name, clean_author)
        
        series_candidates[series_key]['books'].append({
            'ol_key': ol_key,
            'title': title,
            'author': author,
            'isbn': isbn,
            'publication_year': pub_year,
            'confidence': confidence
        })
        
        if author:
            series_candidates[series_key]['authors'].add(author)
        
        series_candidates[series_key]['confidence_scores'].append(confidence)
        series_candidates[series_key]['detection_patterns'].add(strategy)
        
        if pub_year:
            series_candidates[series_key]['publication_years'].append(pub_year)
        
        if isbn:
            series_candidates[series_key]['isbns'].append(isbn)
    
    logger.info(f"üéØ {len(series_candidates)} s√©ries candidates identifi√©es")
    return series_candidates

def validate_and_create_series_permissive(series_candidates):
    """Validation avec crit√®res tr√®s permissifs pour maximiser r√©cup√©ration"""
    valid_series = []
    stats = {
        'total_candidates': len(series_candidates),
        'single_book_accepted': 0,
        'multi_book_high_conf': 0,
        'multi_book_low_conf': 0,
        'rejected_short_name': 0,
        'rejected_low_conf_single': 0
    }
    
    for (series_name, author), data in series_candidates.items():
        book_count = len(data['books'])
        max_confidence = max(data['confidence_scores'])
        avg_confidence = sum(data['confidence_scores']) / len(data['confidence_scores'])
        
        # Crit√®res TR√àS permissifs
        meaningful_name = len(series_name) >= 3 and not series_name.isdigit()
        
        # R√®gles d'acceptation assouplies
        accepted = False
        reason = ""
        
        if not meaningful_name:
            stats['rejected_short_name'] += 1
            continue
        
        if book_count >= 2:
            if max_confidence >= 50:  # Seuil baiss√© de 70 √† 50
                accepted = True
                if max_confidence >= 70:
                    stats['multi_book_high_conf'] += 1
                    reason = "multi_book_high_confidence"
                else:
                    stats['multi_book_low_conf'] += 1
                    reason = "multi_book_medium_confidence"
        
        elif book_count == 1:
            if max_confidence >= 80:  # Seuil √©lev√© pour livres uniques
                accepted = True
                stats['single_book_accepted'] += 1
                reason = "single_book_high_confidence"
            else:
                stats['rejected_low_conf_single'] += 1
                continue
        
        if accepted:
            # Cat√©gorisation intelligente
            category = categorize_series_enhanced(data['books'])
            
            # G√©n√©ration keywords
            keywords = generate_series_keywords_enhanced(series_name, author, data['books'])
            
            # Variations titre
            variations = generate_title_variations_enhanced(series_name)
            
            series_entry = {
                "name": series_name,
                "authors": list(data['authors']),
                "category": category,
                "volumes": book_count,
                "keywords": keywords,
                "variations": variations,
                "exclusions": ["anthology", "collection", "omnibus", "complete"],
                "source": "ultra_harvest_100k_permissive",
                "confidence_score": max_confidence,
                "avg_confidence": avg_confidence,
                "validation_reason": reason,
                "auto_generated": True,
                "detection_date": datetime.now().isoformat(),
                "ultra_harvest_info": {
                    "books_analyzed": len(data['books']),
                    "detection_patterns": list(data['detection_patterns']),
                    "avg_confidence": avg_confidence,
                    "isbn_samples": data['isbns'][:3],
                    "publication_years": list(set(data['publication_years']))
                }
            }
            
            valid_series.append(series_entry)
    
    logger.info(f"‚úÖ {len(valid_series)} s√©ries valid√©es avec crit√®res permissifs")
    logger.info(f"üìä Statistiques validation:")
    for key, value in stats.items():
        logger.info(f"   {key}: {value}")
    
    return valid_series, stats

def categorize_series_enhanced(books):
    """Cat√©gorisation am√©lior√©e avec plus de patterns"""
    all_titles = ' '.join([book['title'].lower() for book in books])
    
    # D√©tection manga (patterns √©tendus)
    manga_indicators = [
        'manga', 'anime', 'light novel', 'vol.', 'tome', 'naruto', 'one piece', 
        'dragon ball', 'bleach', 'fullmetal', 'berserk', 'attack on titan',
        'my hero academia', 'demon slayer', 'jujutsu kaisen', 'chainsaw man',
        'japanese', 'shonen', 'shojo', 'seinen', 'josei'
    ]
    if any(indicator in all_titles for indicator in manga_indicators):
        return 'manga'
    
    # D√©tection BD (patterns √©tendus)
    bd_indicators = [
        'ast√©rix', 'tintin', 'spirou', 'gaston', 'thorgal', 'lucky luke',
        'bd', 'bande dessin√©e', 'comic', 'blake et mortimer', 'yoko tsuno',
        'largo winch', 'tuniques bleues', 'schtroumpfs', 'marsupilami'
    ]
    if any(indicator in all_titles for indicator in bd_indicators):
        return 'bd'
    
    # Par d√©faut: roman
    return 'roman'

def generate_series_keywords_enhanced(series_name, author, books):
    """G√©n√©ration mots-cl√©s am√©lior√©e"""
    keywords = []
    
    # Mots du titre s√©rie (nettoy√©s)
    series_words = re.findall(r'\b\w{3,}\b', series_name.lower())
    keywords.extend(series_words)
    
    # Mots de l'auteur
    if author and author != "Unknown":
        author_words = re.findall(r'\b\w{3,}\b', author.lower())
        keywords.extend(author_words)
    
    # Mots fr√©quents dans titres
    all_titles = ' '.join([book['title'].lower() for book in books])
    title_words = re.findall(r'\b\w{4,}\b', all_titles)
    
    # Compter fr√©quence et ajouter mots fr√©quents
    word_count = defaultdict(int)
    for word in title_words:
        if word not in ['volume', 'tome', 'book', 'part', 'chapter']:
            word_count[word] += 1
    
    frequent_words = [word for word, count in word_count.items() if count >= 2]
    keywords.extend(frequent_words[:8])
    
    # Nettoyer et d√©dupliquer
    keywords = list(set([k for k in keywords if len(k) >= 3 and k.isalpha()]))
    
    return keywords[:12]  # Limite √©tendue √† 12 mots-cl√©s

def generate_title_variations_enhanced(series_name):
    """G√©n√©ration variations √©tendues"""
    variations = [series_name]
    
    # Variations ponctuation
    variations.append(series_name.replace(':', ''))
    variations.append(series_name.replace(' - ', ' '))
    variations.append(series_name.replace('.', ''))
    variations.append(series_name.replace(',', ''))
    
    # Variations articles
    if series_name.startswith('The '):
        variations.append(series_name[4:])
    elif series_name.startswith('Les '):
        variations.append(series_name[4:])
    elif series_name.startswith('La '):
        variations.append(series_name[3:])
    elif series_name.startswith('Le '):
        variations.append(series_name[3:])
    
    # Variations espaces/tirets
    variations.append(series_name.replace(' ', '-'))
    variations.append(series_name.replace('-', ' '))
    
    return list(set([v for v in variations if len(v) >= 3]))

def load_existing_series_with_analysis():
    """Charger s√©ries existantes avec analyse d√©taill√©e"""
    series_path = Path('/app/backend/data/extended_series_database.json')
    
    try:
        with open(series_path, 'r') as f:
            existing_series = json.load(f)
        
        # Cr√©er index d√©taill√©
        existing_index = {}
        existing_names_lower = set()
        
        for series in existing_series:
            name_lower = series['name'].lower().strip()
            existing_names_lower.add(name_lower)
            existing_index[name_lower] = series
        
        logger.info(f"üìö {len(existing_series)} s√©ries existantes charg√©es")
        logger.info(f"üìä {len(existing_names_lower)} noms uniques (apr√®s normalisation)")
        
        return existing_series, existing_index, existing_names_lower
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lecture s√©ries existantes: {e}")
        return [], {}, set()

def integrate_new_series_detailed(new_series, existing_index, existing_names_lower):
    """Int√©gration avec analyse d√©taill√©e des rejets/acceptations"""
    unique_new_series = []
    duplicate_analysis = []
    
    for series in new_series:
        series_name_lower = series['name'].lower().strip()
        
        if series_name_lower not in existing_names_lower:
            unique_new_series.append(series)
        else:
            # Analyser le doublon pour documentation
            existing_series = existing_index[series_name_lower]
            duplicate_analysis.append({
                'new_name': series['name'],
                'existing_name': existing_series['name'],
                'new_confidence': series['confidence_score'],
                'new_volumes': series['volumes'],
                'existing_volumes': existing_series.get('volumes', 'N/A'),
                'new_source': series['source']
            })
    
    logger.info(f"üÜï {len(unique_new_series)} nouvelles s√©ries uniques √† ajouter")
    logger.info(f"üîÑ {len(duplicate_analysis)} doublons d√©tect√©s et ignor√©s")
    
    return unique_new_series, duplicate_analysis

def save_detailed_report(stats, duplicate_analysis, unique_new_series):
    """Sauvegarder rapport d√©taill√© de l'op√©ration"""
    report = {
        "timestamp": datetime.now().isoformat(),
        "operation": "ultra_harvest_integration_permissive",
        "validation_stats": stats,
        "duplicates_found": len(duplicate_analysis),
        "new_series_added": len(unique_new_series),
        "duplicate_samples": duplicate_analysis[:20],  # √âchantillon de doublons
        "new_series_samples": [
            {
                "name": s['name'],
                "category": s['category'],
                "volumes": s['volumes'],
                "confidence": s['confidence_score'],
                "reason": s['validation_reason']
            }
            for s in unique_new_series[:20]
        ]
    }
    
    report_path = Path(f'/app/reports/ultra_harvest_integration_permissive_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
    report_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(report_path, 'w') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    logger.info(f"üìÑ Rapport d√©taill√© sauvegard√©: {report_path}")
    return report_path

def main():
    """Fonction principale avec crit√®res permissifs"""
    logger.info("üöÄ D√âMARRAGE INT√âGRATION ULTRA HARVEST - CRIT√àRES PERMISSIFS")
    
    try:
        # 1. Charger donn√©es tracking (crit√®res assouplis)
        books_data = load_tracking_data_enhanced()
        if not books_data:
            logger.error("‚ùå Aucune donn√©e √† int√©grer")
            return
        
        # 2. Grouper par s√©ries
        series_candidates = group_books_by_series_enhanced(books_data)
        
        # 3. Valider avec crit√®res permissifs
        new_series, validation_stats = validate_and_create_series_permissive(series_candidates)
        
        if not new_series:
            logger.warning("‚ö†Ô∏è Aucune s√©rie valide d√©tect√©e")
            return
        
        # 4. Charger s√©ries existantes avec analyse
        existing_series, existing_index, existing_names_lower = load_existing_series_with_analysis()
        
        # 5. Int√©grer avec analyse d√©taill√©e
        unique_new, duplicate_analysis = integrate_new_series_detailed(
            new_series, existing_index, existing_names_lower
        )
        
        # 6. Sauvegarder rapport avant modification
        report_path = save_detailed_report(validation_stats, duplicate_analysis, unique_new)
        
        if unique_new:
            # 7. Combiner et sauvegarder
            all_series = existing_series + unique_new
            
            # Backup s√©curis√©
            series_path = Path('/app/backend/data/extended_series_database.json')
            backup_path = Path(f'/app/backups/series_detection/backup_before_permissive_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
            backup_path.parent.mkdir(parents=True, exist_ok=True)
            
            with open(series_path, 'r') as f_src, open(backup_path, 'w') as f_dst:
                f_dst.write(f_src.read())
            
            # Sauvegarde base int√©gr√©e
            with open(series_path, 'w') as f:
                json.dump(all_series, f, indent=2, ensure_ascii=False)
            
            logger.info(f"üíæ Backup cr√©√©: {backup_path}")
            logger.info(f"‚úÖ Base de donn√©es mise √† jour: {len(all_series)} s√©ries totales")
        
        # 8. R√©sultats finaux d√©taill√©s
        logger.info(f"""
‚úÖ INT√âGRATION PERMISSIVE TERMIN√âE !
====================================
üìö Livres analys√©s: {len(books_data):,}
üéØ S√©ries candidates: {len(series_candidates):,}
‚úÖ S√©ries valid√©es: {len(new_series):,}
üÜï Nouvelles uniques: {len(unique_new):,}
üîÑ Doublons ignor√©s: {len(duplicate_analysis):,}
üìä Total base finale: {len(existing_series) + len(unique_new):,}
üìÑ Rapport d√©taill√©: {report_path}
====================================
""")
        
        # Afficher √©chantillon nouvelles s√©ries
        if unique_new:
            logger.info("üÜï √âCHANTILLON NOUVELLES S√âRIES AJOUT√âES:")
            for i, series in enumerate(unique_new[:10]):
                logger.info(f"   {i+1:2d}. {series['name']} ({series['category']}, {series['volumes']} vol, conf:{series['confidence_score']:.0f})")
        
    except Exception as e:
        logger.error(f"‚ùå Erreur int√©gration: {e}")
        raise

if __name__ == "__main__":
    main()