#!/usr/bin/env python3
"""
ğŸ”§ INTÃ‰GRATION RÃ‰SULTATS ULTRA HARVEST 100K
Script pour intÃ©grer les sÃ©ries dÃ©tectÃ©es par l'Ultra Harvest dans la base principale
"""

import sqlite3
import json
import logging
from pathlib import Path
from datetime import datetime
from collections import defaultdict
from typing import List, Dict, Set
import re

# Configuration logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def load_tracking_data():
    """Charger donnÃ©es depuis base SQLite tracking"""
    db_path = Path('/app/data/ultra_harvest_tracking.db')
    
    if not db_path.exists():
        logger.error(f"âŒ Base de donnÃ©es tracking non trouvÃ©e: {db_path}")
        return []
    
    try:
        with sqlite3.connect(db_path) as conn:
            # RÃ©cupÃ©rer tous les livres avec sÃ©ries dÃ©tectÃ©es
            cursor = conn.execute("""
                SELECT open_library_key, title, author, series_name, confidence_score, 
                       processing_time_ms, source_strategy, isbn, publication_year
                FROM analyzed_books 
                WHERE series_detected = 1 AND series_name IS NOT NULL
                ORDER BY series_name, title
            """)
            
            books_data = cursor.fetchall()
            logger.info(f"ğŸ“š {len(books_data)} livres avec sÃ©ries dÃ©tectÃ©es rÃ©cupÃ©rÃ©s")
            
            return books_data
            
    except Exception as e:
        logger.error(f"âŒ Erreur lecture base tracking: {e}")
        return []

def group_books_by_series(books_data):
    """Grouper livres par sÃ©rie dÃ©tectÃ©e"""
    series_candidates = defaultdict(lambda: {
        'books': [],
        'authors': set(),
        'confidence_scores': [],
        'detection_patterns': set(),
        'publication_years': [],
        'isbns': []
    })
    
    for book in books_data:
        (ol_key, title, author, series_name, confidence, processing_time, 
         strategy, isbn, pub_year) = book
        
        # ClÃ© unique pour la sÃ©rie
        series_key = (series_name.strip(), author.strip() if author else "Unknown")
        
        series_candidates[series_key]['books'].append({
            'ol_key': ol_key,
            'title': title,
            'author': author,
            'isbn': isbn,
            'publication_year': pub_year
        })
        
        if author:
            series_candidates[series_key]['authors'].add(author)
        
        series_candidates[series_key]['confidence_scores'].append(confidence)
        series_candidates[series_key]['detection_patterns'].add(strategy)
        
        if pub_year:
            series_candidates[series_key]['publication_years'].append(pub_year)
        
        if isbn:
            series_candidates[series_key]['isbns'].append(isbn)
    
    logger.info(f"ğŸ¯ {len(series_candidates)} sÃ©ries candidates identifiÃ©es")
    return series_candidates

def validate_and_create_series(series_candidates):
    """Valider et crÃ©er entrÃ©es sÃ©rie"""
    valid_series = []
    
    for (series_name, author), data in series_candidates.items():
        # CritÃ¨res validation
        has_multiple_books = len(data['books']) >= 2
        good_confidence = max(data['confidence_scores']) >= 70  # Seuil plus bas pour rÃ©cupÃ©rer plus
        meaningful_name = len(series_name) >= 3 and not series_name.isdigit()
        
        if has_multiple_books and good_confidence and meaningful_name:
            # CatÃ©gorisation intelligente
            category = categorize_series(data['books'])
            
            # GÃ©nÃ©ration keywords
            keywords = generate_series_keywords(series_name, author, data['books'])
            
            # Variations titre
            variations = generate_title_variations(series_name)
            
            series_entry = {
                "name": series_name,
                "authors": list(data['authors']),
                "category": category,
                "volumes": len(data['books']),
                "keywords": keywords,
                "variations": variations,
                "exclusions": ["anthology", "collection", "omnibus", "complete"],
                "source": "ultra_harvest_100k",
                "confidence_score": max(data['confidence_scores']),
                "auto_generated": True,
                "detection_date": datetime.now().isoformat(),
                "ultra_harvest_info": {
                    "books_analyzed": len(data['books']),
                    "detection_patterns": list(data['detection_patterns']),
                    "avg_confidence": sum(data['confidence_scores']) / len(data['confidence_scores']),
                    "isbn_samples": data['isbns'][:3],
                    "publication_years": list(set(data['publication_years']))
                }
            }
            
            valid_series.append(series_entry)
    
    logger.info(f"âœ… {len(valid_series)} sÃ©ries validÃ©es pour intÃ©gration")
    return valid_series

def categorize_series(books):
    """CatÃ©gorisation intelligente sÃ©rie"""
    # Analyse titres pour indices catÃ©gorie
    all_titles = ' '.join([book['title'].lower() for book in books])
    
    # DÃ©tection manga
    manga_indicators = ['manga', 'anime', 'light novel', 'vol.', 'tome', 'naruto', 'one piece', 'dragon ball']
    if any(indicator in all_titles for indicator in manga_indicators):
        return 'manga'
    
    # DÃ©tection BD
    bd_indicators = ['astÃ©rix', 'tintin', 'spirou', 'gaston', 'bd', 'bande dessinÃ©e', 'comic']
    if any(indicator in all_titles for indicator in bd_indicators):
        return 'bd'
    
    # Par dÃ©faut: roman
    return 'roman'

def generate_series_keywords(series_name, author, books):
    """GÃ©nÃ©ration mots-clÃ©s sÃ©rie"""
    keywords = []
    
    # Mots du titre sÃ©rie
    series_words = re.findall(r'\b\w{3,}\b', series_name.lower())
    keywords.extend(series_words)
    
    # Mots de l'auteur
    if author and author != "Unknown":
        author_words = re.findall(r'\b\w{3,}\b', author.lower())
        keywords.extend(author_words)
    
    # Mots frÃ©quents dans titres
    all_titles = ' '.join([book['title'].lower() for book in books])
    title_words = re.findall(r'\b\w{4,}\b', all_titles)
    
    # Compter frÃ©quence
    word_count = defaultdict(int)
    for word in title_words:
        word_count[word] += 1
    
    # Ajouter mots frÃ©quents
    frequent_words = [word for word, count in word_count.items() if count >= 2]
    keywords.extend(frequent_words[:5])
    
    # Nettoyer et dÃ©dupliquer
    keywords = list(set([k for k in keywords if len(k) >= 3]))
    
    return keywords[:10]  # Limite Ã  10 mots-clÃ©s

def generate_title_variations(series_name):
    """GÃ©nÃ©ration variations titre"""
    variations = [series_name]
    
    # Variations ponctuation
    variations.append(series_name.replace(':', ''))
    variations.append(series_name.replace(' - ', ' '))
    
    # Variations articles
    if series_name.startswith('The '):
        variations.append(series_name[4:])
    elif series_name.startswith('Les '):
        variations.append(series_name[4:])
    
    return list(set(variations))

def load_existing_series():
    """Charger sÃ©ries existantes"""
    series_path = Path('/app/backend/data/extended_series_database.json')
    
    if not series_path.exists():
        logger.warning("âš ï¸ Fichier sÃ©ries existant non trouvÃ©, crÃ©ation nouveau")
        return []
    
    try:
        with open(series_path, 'r') as f:
            existing_series = json.load(f)
        
        logger.info(f"ğŸ“š {len(existing_series)} sÃ©ries existantes chargÃ©es")
        return existing_series
        
    except Exception as e:
        logger.error(f"âŒ Erreur lecture sÃ©ries existantes: {e}")
        return []

def integrate_new_series(new_series, existing_series):
    """IntÃ©grer nouvelles sÃ©ries en Ã©vitant doublons"""
    # CrÃ©er index sÃ©ries existantes
    existing_names = {series['name'].lower() for series in existing_series}
    
    # Filtrer nouvelles sÃ©ries
    unique_new_series = []
    for series in new_series:
        if series['name'].lower() not in existing_names:
            unique_new_series.append(series)
    
    logger.info(f"ğŸ†• {len(unique_new_series)} nouvelles sÃ©ries uniques Ã  ajouter")
    
    # Combiner toutes les sÃ©ries
    all_series = existing_series + unique_new_series
    
    return all_series, unique_new_series

def save_integrated_database(all_series):
    """Sauvegarder base de donnÃ©es intÃ©grÃ©e"""
    series_path = Path('/app/backend/data/extended_series_database.json')
    
    # Backup de sÃ©curitÃ©
    if series_path.exists():
        backup_path = Path(f'/app/backups/series_detection/backup_before_ultra_harvest_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        backup_path.parent.mkdir(parents=True, exist_ok=True)
        
        with open(series_path, 'r') as f_src, open(backup_path, 'w') as f_dst:
            f_dst.write(f_src.read())
        
        logger.info(f"ğŸ’¾ Backup crÃ©Ã©: {backup_path}")
    
    # Sauvegarde base intÃ©grÃ©e
    try:
        with open(series_path, 'w') as f:
            json.dump(all_series, f, indent=2, ensure_ascii=False)
        
        logger.info(f"âœ… Base de donnÃ©es mise Ã  jour: {len(all_series)} sÃ©ries totales")
        
    except Exception as e:
        logger.error(f"âŒ Erreur sauvegarde: {e}")
        raise

def main():
    """Fonction principale intÃ©gration"""
    logger.info("ğŸš€ DÃ‰MARRAGE INTÃ‰GRATION ULTRA HARVEST 100K")
    
    try:
        # 1. Charger donnÃ©es tracking
        books_data = load_tracking_data()
        if not books_data:
            logger.error("âŒ Aucune donnÃ©e Ã  intÃ©grer")
            return
        
        # 2. Grouper par sÃ©ries
        series_candidates = group_books_by_series(books_data)
        
        # 3. Valider et crÃ©er sÃ©ries
        new_series = validate_and_create_series(series_candidates)
        
        if not new_series:
            logger.warning("âš ï¸ Aucune sÃ©rie valide dÃ©tectÃ©e")
            return
        
        # 4. Charger sÃ©ries existantes
        existing_series = load_existing_series()
        
        # 5. IntÃ©grer nouvelles sÃ©ries
        all_series, unique_new = integrate_new_series(new_series, existing_series)
        
        # 6. Sauvegarder base intÃ©grÃ©e
        save_integrated_database(all_series)
        
        # 7. RÃ©sultats finaux
        logger.info(f"""
âœ… INTÃ‰GRATION ULTRA HARVEST TERMINÃ‰E !
======================================
ğŸ“š Livres analysÃ©s: {len(books_data):,}
ğŸ¯ SÃ©ries candidates: {len(series_candidates):,}
âœ… SÃ©ries validÃ©es: {len(new_series):,}
ğŸ†• Nouvelles ajoutÃ©es: {len(unique_new):,}
ğŸ“Š Total base: {len(all_series):,}
======================================
""")
        
    except Exception as e:
        logger.error(f"âŒ Erreur intÃ©gration: {e}")
        raise

if __name__ == "__main__":
    main()