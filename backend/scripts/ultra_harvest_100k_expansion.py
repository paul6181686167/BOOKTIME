#!/usr/bin/env python3
"""
ğŸš€ ULTRA HARVEST 100K AUTOEXPANSION OPENLIBRARY
Script avancÃ© pour l'expansion massive des sÃ©ries BOOKTIME

FonctionnalitÃ©s :
- Traitement du fichier wikidata_new_series_discovery.json
- Ajout sÃ©ries Robert Muchamore (CHERUB, Henderson's Boys)
- Ajout sÃ©rie Red Rising (Pierce Brown)
- Expansion ultra-rapide 100k+ sÃ©ries OpenLibrary
- Sauvegarde automatique avec backup
- Statistiques dÃ©taillÃ©es

Utilisation :
python ultra_harvest_100k_expansion.py --wikidata --muchamore --red-rising --expansion-limit=1000
"""

import asyncio
import aiohttp
import json
import re
import logging
from typing import List, Dict, Optional, Set
from datetime import datetime
from pathlib import Path
import argparse
import uuid

# Configuration logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('/app/logs/ultra_harvest_100k.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class UltraHarvest100kExpansion:
    """Expansion massive des sÃ©ries avec mÃ©thode Ultra Harvest 100k"""
    
    def __init__(self):
        self.session = None
        self.series_database_path = Path('/app/backend/data/extended_series_database.json')
        self.wikidata_file_path = Path('/app/backend/wikidata_new_series_discovery.json')
        self.backup_dir = Path('/app/backend/data/backups')
        self.backup_dir.mkdir(exist_ok=True)
        
        self.stats = {
            'wikidata_processed': 0,
            'wikidata_added': 0,
            'muchamore_added': 0,
            'red_rising_added': 0,
            'expansion_processed': 0,
            'expansion_added': 0,
            'total_added': 0,
            'duplicates_skipped': 0,
            'errors': 0
        }
        
        self.existing_series = set()
        self.new_series = []
    
    async def __aenter__(self):
        """Initialisation session async"""
        self.session = aiohttp.ClientSession(
            timeout=aiohttp.ClientTimeout(total=30),
            headers={'User-Agent': 'BOOKTIME-UltraHarvest/2.0'}
        )
        return self
    
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """Fermeture session"""
        if self.session:
            await self.session.close()
    
    def load_existing_series(self) -> Set[str]:
        """Charger les sÃ©ries existantes pour Ã©viter les doublons"""
        try:
            if not self.series_database_path.exists():
                logger.warning("Base de donnÃ©es des sÃ©ries non trouvÃ©e, crÃ©ation d'une nouvelle")
                return set()
            
            with open(self.series_database_path, 'r', encoding='utf-8') as f:
                existing_data = json.load(f)
            
            existing_names = set()
            for series in existing_data:
                existing_names.add(series['name'].lower().strip())
                # Ajouter variations
                for variation in series.get('variations', []):
                    existing_names.add(variation.lower().strip())
            
            logger.info(f"âœ… ChargÃ© {len(existing_names)} noms/variations de sÃ©ries existantes")
            return existing_names
        
        except Exception as e:
            logger.error(f"âŒ Erreur chargement sÃ©ries existantes: {e}")
            return set()
    
    def is_duplicate(self, series_name: str) -> bool:
        """VÃ©rifier si une sÃ©rie est un doublon"""
        name_lower = series_name.lower().strip()
        
        # VÃ©rification exacte
        if name_lower in self.existing_series:
            return True
        
        # VÃ©rification similaritÃ©
        name_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', name_lower))
        
        for existing_name in self.existing_series:
            existing_words = set(re.findall(r'\b[a-zA-Z]{3,}\b', existing_name))
            
            if len(name_words) > 0 and len(existing_words) > 0:
                common_words = name_words.intersection(existing_words)
                similarity = len(common_words) / max(len(name_words), len(existing_words))
                
                if similarity > 0.85:  # Seuil Ã©levÃ© pour Ã©viter faux positifs
                    return True
        
        return False
    
    def create_backup(self):
        """CrÃ©er une sauvegarde de la base de donnÃ©es actuelle"""
        try:
            if not self.series_database_path.exists():
                logger.info("Aucune base existante Ã  sauvegarder")
                return
            
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_file = self.backup_dir / f"extended_series_database_backup_ultraharvest_{timestamp}.json"
            
            import shutil
            shutil.copy2(self.series_database_path, backup_file)
            
            logger.info(f"âœ… Sauvegarde crÃ©Ã©e: {backup_file}")
        
        except Exception as e:
            logger.error(f"âŒ Erreur crÃ©ation sauvegarde: {e}")
    
    def process_wikidata_discovery(self) -> int:
        """Traiter le fichier wikidata_new_series_discovery.json"""
        try:
            if not self.wikidata_file_path.exists():
                logger.warning("Fichier wikidata_new_series_discovery.json non trouvÃ©")
                return 0
            
            with open(self.wikidata_file_path, 'r', encoding='utf-8') as f:
                wikidata_data = json.load(f)
            
            new_series_data = wikidata_data.get('new_series', [])
            logger.info(f"ğŸ” Traitement de {len(new_series_data)} sÃ©ries Wikidata")
            
            added_count = 0
            for series_data in new_series_data:
                self.stats['wikidata_processed'] += 1
                
                series_name = series_data.get('name', '').strip()
                if not series_name or self.is_duplicate(series_name):
                    self.stats['duplicates_skipped'] += 1
                    continue
                
                # Conversion au format BOOKTIME
                booktime_series = {
                    'name': series_name,
                    'authors': [series_data.get('author', 'Unknown')],
                    'category': self.map_wikidata_category(series_data.get('genre', '')),
                    'volumes': 1,  # Sera mis Ã  jour par recherche
                    'keywords': self.extract_keywords_from_wikidata(series_data),
                    'variations': self.generate_variations(series_name),
                    'first_published': None,
                    'status': series_data.get('status', 'ongoing'),
                    'description': series_data.get('description', ''),
                    'source': 'wikidata_discovery',
                    'wikidata_id': series_data.get('wikidata_id'),
                    'subjects': [series_data.get('genre', '')],
                    'languages': ['en', 'fr'],
                    'translations': {
                        'en': series_name,
                        'fr': series_name
                    },
                    'ultra_harvest_info': {
                        'processed_date': datetime.now().isoformat(),
                        'source_file': 'wikidata_new_series_discovery.json'
                    }
                }
                
                self.new_series.append(booktime_series)
                self.existing_series.add(series_name.lower().strip())
                added_count += 1
                self.stats['wikidata_added'] += 1
            
            logger.info(f"âœ… AjoutÃ© {added_count} sÃ©ries depuis Wikidata")
            return added_count
        
        except Exception as e:
            logger.error(f"âŒ Erreur traitement Wikidata: {e}")
            self.stats['errors'] += 1
            return 0
    
    def map_wikidata_category(self, genre: str) -> str:
        """Mapper les genres Wikidata vers les catÃ©gories BOOKTIME"""
        genre_lower = genre.lower()
        
        if any(keyword in genre_lower for keyword in ['fantasy', 'science-fiction', 'littÃ©rature', 'roman', 'fiction']):
            return 'roman'
        elif any(keyword in genre_lower for keyword in ['comic', 'graphic', 'bd', 'bande dessinÃ©e']):
            return 'bd'
        elif any(keyword in genre_lower for keyword in ['manga', 'anime', 'japanese']):
            return 'manga'
        else:
            return 'roman'  # DÃ©faut
    
    def extract_keywords_from_wikidata(self, series_data: Dict) -> List[str]:
        """Extraire mots-clÃ©s depuis donnÃ©es Wikidata"""
        keywords = []
        
        # Mots-clÃ©s du nom
        name = series_data.get('name', '')
        name_words = re.findall(r'\b[a-zA-Z]{3,}\b', name.lower())
        keywords.extend(name_words[:5])
        
        # Mots-clÃ©s de l'auteur
        author = series_data.get('author', '')
        author_words = re.findall(r'\b[a-zA-Z]{3,}\b', author.lower())
        keywords.extend(author_words[:2])
        
        # Mots-clÃ©s du genre
        genre = series_data.get('genre', '')
        genre_words = re.findall(r'\b[a-zA-Z]{3,}\b', genre.lower())
        keywords.extend(genre_words[:2])
        
        # DÃ©duplication
        return list(set(keywords))
    
    def add_robert_muchamore_series(self) -> int:
        """Ajouter les sÃ©ries de Robert Muchamore"""
        try:
            muchamore_series = [
                {
                    'name': 'CHERUB',
                    'authors': ['Robert Muchamore'],
                    'category': 'roman',
                    'volumes': 17,
                    'keywords': ['cherub', 'espionnage', 'adolescent', 'agent', 'mission', 'robert'],
                    'variations': ['CHERUB', 'Cherub', 'CHERUB Series'],
                    'first_published': 2004,
                    'status': 'completed',
                    'description': 'SÃ©rie d\'espionnage mettant en scÃ¨ne des adolescents agents secrets',
                    'source': 'manual_muchamore',
                    'subjects': ['spy fiction', 'young adult', 'action', 'thriller'],
                    'languages': ['en', 'fr'],
                    'translations': {
                        'en': 'CHERUB',
                        'fr': 'CHERUB'
                    },
                    'ultra_harvest_info': {
                        'processed_date': datetime.now().isoformat(),
                        'manual_addition': True,
                        'user_requested': True
                    }
                },
                {
                    'name': 'Henderson\'s Boys',
                    'authors': ['Robert Muchamore'],
                    'category': 'roman',
                    'volumes': 7,
                    'keywords': ['henderson', 'boys', 'guerre', 'mondiale', 'resistance', 'robert'],
                    'variations': ['Henderson\'s Boys', 'Hendersons Boys', 'Henderson Boys'],
                    'first_published': 2009,
                    'status': 'completed',
                    'description': 'SÃ©rie se dÃ©roulant pendant la Seconde Guerre mondiale, prÃ©quelle Ã  CHERUB',
                    'source': 'manual_muchamore',
                    'subjects': ['world war 2', 'historical fiction', 'young adult', 'resistance'],
                    'languages': ['en', 'fr'],
                    'translations': {
                        'en': 'Henderson\'s Boys',
                        'fr': 'Henderson\'s Boys'
                    },
                    'ultra_harvest_info': {
                        'processed_date': datetime.now().isoformat(),
                        'manual_addition': True,
                        'user_requested': True
                    }
                }
            ]
            
            added_count = 0
            for series_data in muchamore_series:
                if not self.is_duplicate(series_data['name']):
                    self.new_series.append(series_data)
                    self.existing_series.add(series_data['name'].lower().strip())
                    added_count += 1
                    self.stats['muchamore_added'] += 1
                else:
                    self.stats['duplicates_skipped'] += 1
            
            logger.info(f"âœ… AjoutÃ© {added_count} sÃ©ries de Robert Muchamore")
            return added_count
        
        except Exception as e:
            logger.error(f"âŒ Erreur ajout sÃ©ries Muchamore: {e}")
            self.stats['errors'] += 1
            return 0
    
    def add_red_rising_series(self) -> int:
        """Ajouter la sÃ©rie Red Rising"""
        try:
            red_rising_series = {
                'name': 'Red Rising',
                'authors': ['Pierce Brown'],
                'category': 'roman',
                'volumes': 6,  # Mise Ã  jour 2025
                'keywords': ['red', 'rising', 'dystopia', 'science', 'fiction', 'pierce', 'brown'],
                'variations': ['Red Rising', 'Red Rising Saga', 'Red Rising Series'],
                'first_published': 2014,
                'status': 'ongoing',
                'description': 'Saga science-fiction dystopique de Pierce Brown',
                'source': 'manual_red_rising',
                'subjects': ['science fiction', 'dystopian', 'space opera', 'young adult'],
                'languages': ['en', 'fr'],
                'translations': {
                    'en': 'Red Rising',
                    'fr': 'Red Rising'
                },
                'ultra_harvest_info': {
                    'processed_date': datetime.now().isoformat(),
                    'manual_addition': True,
                    'user_requested': True,
                    'volumes_info': {
                        'total_planned': 7,
                        'published': 6,
                        'next_release': 'Red God (2026)'
                    }
                }
            }
            
            if not self.is_duplicate(red_rising_series['name']):
                self.new_series.append(red_rising_series)
                self.existing_series.add(red_rising_series['name'].lower().strip())
                self.stats['red_rising_added'] += 1
                logger.info("âœ… AjoutÃ© sÃ©rie Red Rising")
                return 1
            else:
                self.stats['duplicates_skipped'] += 1
                logger.info("â­ï¸ SÃ©rie Red Rising dÃ©jÃ  existante")
                return 0
        
        except Exception as e:
            logger.error(f"âŒ Erreur ajout sÃ©rie Red Rising: {e}")
            self.stats['errors'] += 1
            return 0
    
    async def ultra_expansion_openlibrary(self, limit: int = 1000) -> int:
        """Expansion ultra-rapide depuis OpenLibrary"""
        try:
            logger.info(f"ğŸš€ DÃ©marrage Ultra Expansion OpenLibrary (limite: {limit})")
            
            # Termes de recherche optimisÃ©s pour sÃ©ries
            search_terms = [
                'book series fantasy', 'manga series', 'comic series', 'novel series',
                'fantasy trilogy', 'science fiction series', 'mystery series',
                'thriller series', 'adventure series', 'romance series',
                'light novel series', 'graphic novel series', 'dystopian series',
                'urban fantasy series', 'epic fantasy series', 'space opera series'
            ]
            
            added_count = 0
            processed_count = 0
            
            for term in search_terms[:8]:  # Limite Ã  8 termes pour Ã©viter rate limiting
                try:
                    logger.info(f"ğŸ” Recherche: {term}")
                    
                    url = "https://openlibrary.org/search.json"
                    params = {
                        'q': term,
                        'limit': limit // len(search_terms[:8]) + 10,
                        'has_fulltext': 'true',
                        'sort': 'rating'
                    }
                    
                    async with self.session.get(url, params=params) as response:
                        if response.status != 200:
                            logger.error(f"âŒ Erreur recherche {term}: {response.status}")
                            continue
                        
                        data = await response.json()
                        works = data.get('docs', [])
                        
                        for work in works:
                            processed_count += 1
                            self.stats['expansion_processed'] += 1
                            
                            if processed_count >= limit:
                                break
                            
                            series_info = self.parse_openlibrary_work(work)
                            if series_info and not self.is_duplicate(series_info['name']):
                                self.new_series.append(series_info)
                                self.existing_series.add(series_info['name'].lower().strip())
                                added_count += 1
                                self.stats['expansion_added'] += 1
                            else:
                                self.stats['duplicates_skipped'] += 1
                    
                    # Rate limiting
                    await asyncio.sleep(0.5)
                    
                    if processed_count >= limit:
                        break
                
                except Exception as e:
                    logger.error(f"âŒ Erreur recherche terme {term}: {e}")
                    continue
            
            logger.info(f"âœ… Ultra Expansion terminÃ©e: {added_count} sÃ©ries ajoutÃ©es")
            return added_count
        
        except Exception as e:
            logger.error(f"âŒ Erreur Ultra Expansion: {e}")
            self.stats['errors'] += 1
            return 0
    
    def parse_openlibrary_work(self, work: Dict) -> Optional[Dict]:
        """Parser une Å“uvre OpenLibrary pour extraire une sÃ©rie"""
        try:
            title = work.get('title', '').strip()
            authors = work.get('author_name', [])
            if not title or not authors:
                return None
            
            # Patterns de dÃ©tection de sÃ©rie
            series_patterns = [
                r'(.+?)\s+(?:Book|Volume|Vol\.?|Tome|#)\s*(\d+)',
                r'(.+?)\s+(\d+)(?:st|nd|rd|th)?\s*(?:Book|Volume|Tome)',
                r'(.+?):\s*(.+)',
                r'(.+?)\s+\((.+?)\s+#\d+\)',
                r'(.+?)\s+-\s+(.+)'
            ]
            
            series_name = None
            for pattern in series_patterns:
                match = re.search(pattern, title, re.IGNORECASE)
                if match:
                    series_name = match.group(1).strip()
                    break
            
            if not series_name:
                # VÃ©rification indicateurs sÃ©rie
                series_indicators = [
                    'saga', 'series', 'cycle', 'chronicles', 'adventures',
                    'trilogy', 'quartet', 'collection', 'omnibus'
                ]
                
                if any(indicator in title.lower() for indicator in series_indicators):
                    series_name = title
                else:
                    return None
            
            # MÃ©tadonnÃ©es
            subjects = work.get('subject', [])
            first_published = work.get('first_publish_year')
            
            # DÃ©tection catÃ©gorie
            category = self.detect_category_from_subjects(subjects)
            
            return {
                'name': series_name,
                'authors': authors[:2],  # Limite Ã  2 auteurs
                'category': category,
                'volumes': 1,  # Sera agrÃ©gÃ© plus tard
                'keywords': self.extract_keywords_from_work(title, subjects),
                'variations': self.generate_variations(series_name),
                'first_published': first_published,
                'status': 'ongoing',
                'source': 'ultra_expansion_openlibrary',
                'ol_key': work.get('key'),
                'description': '',
                'subjects': subjects[:8],
                'languages': work.get('language', ['en']),
                'translations': {
                    'en': series_name,
                    'fr': series_name
                },
                'ultra_harvest_info': {
                    'processed_date': datetime.now().isoformat(),
                    'method': 'ultra_expansion_100k',
                    'confidence_score': 85
                }
            }
        
        except Exception as e:
            logger.error(f"âŒ Erreur parsing work: {e}")
            return None
    
    def detect_category_from_subjects(self, subjects: List[str]) -> str:
        """DÃ©tecter catÃ©gorie depuis les sujets"""
        text = ' '.join(subjects).lower()
        
        # Patterns manga
        if any(keyword in text for keyword in ['manga', 'anime', 'japanese comics', 'light novel']):
            return 'manga'
        
        # Patterns BD
        if any(keyword in text for keyword in ['comic', 'graphic novel', 'superhero', 'cartoons']):
            return 'bd'
        
        # DÃ©faut roman
        return 'roman'
    
    def extract_keywords_from_work(self, title: str, subjects: List[str]) -> List[str]:
        """Extraire mots-clÃ©s depuis un work OpenLibrary"""
        keywords = []
        
        # Mots-clÃ©s du titre
        title_words = re.findall(r'\b[a-zA-Z]{3,}\b', title.lower())
        keywords.extend(title_words[:5])
        
        # Mots-clÃ©s des sujets
        for subject in subjects[:3]:
            subject_words = re.findall(r'\b[a-zA-Z]{3,}\b', subject.lower())
            keywords.extend(subject_words[:2])
        
        # DÃ©duplication et nettoyage
        common_words = {
            'the', 'and', 'for', 'are', 'but', 'not', 'you', 'all',
            'can', 'had', 'her', 'was', 'one', 'our', 'out', 'day',
            'book', 'novel', 'story', 'series', 'volume', 'tome'
        }
        
        keywords = [kw for kw in list(set(keywords)) if kw not in common_words]
        return keywords[:10]
    
    def generate_variations(self, series_name: str) -> List[str]:
        """GÃ©nÃ©rer variations nom sÃ©rie"""
        variations = [series_name]
        
        # Variations avec/sans articles
        articles = ['the', 'a', 'an', 'le', 'la', 'les', 'un', 'une', 'des']
        for article in articles:
            variations.append(f"{article} {series_name}")
            variations.append(f"{article.title()} {series_name}")
        
        # Variations casse
        variations.extend([
            series_name.lower(),
            series_name.upper(),
            series_name.title()
        ])
        
        return list(set(variations))
    
    def save_series_database(self) -> bool:
        """Sauvegarder la base de donnÃ©es mise Ã  jour"""
        try:
            # Charger sÃ©ries existantes
            existing_series = []
            if self.series_database_path.exists():
                with open(self.series_database_path, 'r', encoding='utf-8') as f:
                    existing_series = json.load(f)
            
            # Ajouter nouvelles sÃ©ries
            updated_series = existing_series + self.new_series
            
            # CrÃ©ation dossier parent si nÃ©cessaire
            self.series_database_path.parent.mkdir(parents=True, exist_ok=True)
            
            # Sauvegarde avec formatage
            with open(self.series_database_path, 'w', encoding='utf-8') as f:
                json.dump(updated_series, f, indent=2, ensure_ascii=False)
            
            logger.info(f"âœ… Base de donnÃ©es sauvegardÃ©e: {len(updated_series)} sÃ©ries total")
            return True
        
        except Exception as e:
            logger.error(f"âŒ Erreur sauvegarde base: {e}")
            return False
    
    def generate_report(self) -> str:
        """GÃ©nÃ©rer rapport d'exÃ©cution"""
        total_added = sum([
            self.stats['wikidata_added'],
            self.stats['muchamore_added'],
            self.stats['red_rising_added'],
            self.stats['expansion_added']
        ])
        
        report = f"""
ğŸš€ ULTRA HARVEST 100K AUTOEXPANSION - RAPPORT FINAL
==================================================

ğŸ“Š STATISTIQUES GÃ‰NÃ‰RALES
- Total sÃ©ries ajoutÃ©es: {total_added}
- Doublons ignorÃ©s: {self.stats['duplicates_skipped']}
- Erreurs rencontrÃ©es: {self.stats['errors']}

ğŸ“‹ DÃ‰TAIL PAR SOURCE
- Wikidata discovery: {self.stats['wikidata_added']} ({self.stats['wikidata_processed']} traitÃ©es)
- Robert Muchamore: {self.stats['muchamore_added']} (CHERUB + Henderson's Boys)
- Red Rising: {self.stats['red_rising_added']} (Pierce Brown)
- Ultra Expansion: {self.stats['expansion_added']} ({self.stats['expansion_processed']} traitÃ©es)

ğŸ¯ SÃ‰RIES AJOUTÃ‰ES TOP 10
"""
        
        # Top 10 par nombre de volumes
        top_series = sorted(self.new_series, key=lambda x: x.get('volumes', 0), reverse=True)[:10]
        
        for i, series in enumerate(top_series, 1):
            authors = ', '.join(series['authors'][:2])
            volumes = series.get('volumes', 1)
            report += f"{i}. {series['name']} - {authors} ({volumes} vol, {series['category']})\n"
        
        report += f"""
ğŸ“Š RÃ‰PARTITION PAR CATÃ‰GORIE
"""
        
        # Comptage par catÃ©gorie
        categories = {}
        for series in self.new_series:
            cat = series['category']
            categories[cat] = categories.get(cat, 0) + 1
        
        for cat, count in sorted(categories.items()):
            report += f"- {cat.title()}: {count} sÃ©ries\n"
        
        report += f"""
âš¡ PERFORMANCE
- DurÃ©e traitement: {datetime.now().strftime('%H:%M:%S')}
- SÃ©ries par seconde: {total_added / max(1, 60):.1f}
- Taux de succÃ¨s: {(total_added / max(1, total_added + self.stats['duplicates_skipped'])) * 100:.1f}%

ğŸš€ PROCHAINES Ã‰TAPES
- RedÃ©marrage services recommandÃ©
- VÃ©rification dÃ©tection automatique
- Test masquage intelligent
- Validation interface utilisateur

ğŸ“ FICHIERS GÃ‰NÃ‰RÃ‰S
- Base de donnÃ©es: {self.series_database_path}
- Sauvegardes: {self.backup_dir}
- Logs: /app/logs/ultra_harvest_100k.log
"""
        
        return report
    
    async def run_ultra_harvest(self, 
                              process_wikidata: bool = True,
                              add_muchamore: bool = True,
                              add_red_rising: bool = True,
                              expansion_limit: int = 1000) -> Dict:
        """ExÃ©cution complÃ¨te Ultra Harvest 100k"""
        
        start_time = datetime.now()
        logger.info("ğŸš€ DÃ‰MARRAGE ULTRA HARVEST 100K AUTOEXPANSION")
        
        # Ã‰tape 1: Chargement sÃ©ries existantes
        self.existing_series = self.load_existing_series()
        
        # Ã‰tape 2: CrÃ©ation sauvegarde
        self.create_backup()
        
        total_added = 0
        
        # Ã‰tape 3: Traitement Wikidata
        if process_wikidata:
            logger.info("ğŸ“Š PHASE 1: Traitement Wikidata Discovery")
            wikidata_added = self.process_wikidata_discovery()
            total_added += wikidata_added
        
        # Ã‰tape 4: Ajout sÃ©ries Muchamore
        if add_muchamore:
            logger.info("ğŸ“š PHASE 2: Ajout sÃ©ries Robert Muchamore")
            muchamore_added = self.add_robert_muchamore_series()
            total_added += muchamore_added
        
        # Ã‰tape 5: Ajout sÃ©rie Red Rising
        if add_red_rising:
            logger.info("ğŸ”´ PHASE 3: Ajout sÃ©rie Red Rising")
            red_rising_added = self.add_red_rising_series()
            total_added += red_rising_added
        
        # Ã‰tape 6: Ultra Expansion OpenLibrary
        if expansion_limit > 0:
            logger.info("ğŸŒ PHASE 4: Ultra Expansion OpenLibrary")
            expansion_added = await self.ultra_expansion_openlibrary(expansion_limit)
            total_added += expansion_added
        
        # Ã‰tape 7: Sauvegarde
        logger.info("ğŸ’¾ PHASE 5: Sauvegarde base de donnÃ©es")
        save_success = self.save_series_database()
        
        # Ã‰tape 8: GÃ©nÃ©ration rapport
        report = self.generate_report()
        
        # Sauvegarde rapport
        report_file = Path(f'/app/logs/ultra_harvest_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.txt')
        report_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report)
        
        duration = datetime.now() - start_time
        logger.info(f"âœ… ULTRA HARVEST TERMINÃ‰: {total_added} sÃ©ries ajoutÃ©es en {duration}")
        
        return {
            'success': save_success,
            'total_added': total_added,
            'duration': str(duration),
            'report_file': str(report_file),
            'stats': self.stats,
            'new_series_count': len(self.new_series)
        }


async def main():
    """Fonction principale CLI"""
    parser = argparse.ArgumentParser(description='Ultra Harvest 100k AutoExpansion')
    parser.add_argument('--wikidata', action='store_true', help='Traiter fichier Wikidata')
    parser.add_argument('--muchamore', action='store_true', help='Ajouter sÃ©ries Robert Muchamore')
    parser.add_argument('--red-rising', action='store_true', help='Ajouter sÃ©rie Red Rising')
    parser.add_argument('--expansion-limit', type=int, default=1000, help='Limite expansion OpenLibrary')
    parser.add_argument('--all', action='store_true', help='Activer toutes les options')
    
    args = parser.parse_args()
    
    # Si --all, activer toutes les options
    if args.all:
        args.wikidata = True
        args.muchamore = True
        args.red_rising = True
    
    # VÃ©rifier qu'au moins une option est activÃ©e
    if not any([args.wikidata, args.muchamore, args.red_rising, args.expansion_limit > 0]):
        print("âŒ Au moins une option doit Ãªtre activÃ©e")
        parser.print_help()
        return 1
    
    # CrÃ©ation dossiers
    Path('/app/logs').mkdir(exist_ok=True)
    Path('/app/backend/data').mkdir(parents=True, exist_ok=True)
    
    # ExÃ©cution
    async with UltraHarvest100kExpansion() as harvester:
        result = await harvester.run_ultra_harvest(
            process_wikidata=args.wikidata,
            add_muchamore=args.muchamore,
            add_red_rising=args.red_rising,
            expansion_limit=args.expansion_limit
        )
        
        print(f"\nğŸ¯ RÃ‰SULTATS ULTRA HARVEST 100K")
        print(f"âœ… SÃ©ries ajoutÃ©es: {result['total_added']}")
        print(f"â±ï¸ DurÃ©e: {result['duration']}")
        print(f"ğŸ“Š DÃ©tail: {result['stats']}")
        print(f"ğŸ“„ Rapport: {result['report_file']}")
        
        if result['success']:
            print(f"\nğŸš€ {result['total_added']} nouvelles sÃ©ries ajoutÃ©es Ã  BOOKTIME !")
            print(f"ğŸ”„ RedÃ©marrage services recommandÃ©")
        else:
            print(f"\nâŒ Erreur lors de la sauvegarde")
            return 1
    
    return 0


if __name__ == "__main__":
    exit(asyncio.run(main()))