"""
Routes pour l'API Wikipedia
RÃ©cupÃ©ration d'informations sur les auteurs avec donnÃ©es curÃ©es
"""

from fastapi import APIRouter, HTTPException
import httpx
import re
from typing import Optional, List, Dict
import logging

router = APIRouter(prefix="/api/wikipedia", tags=["wikipedia"])

logger = logging.getLogger(__name__)

async def get_wikipedia_full_content(author_name: str) -> Optional[dict]:
    """
    RÃ©cupÃ©rer le contenu complet d'une page Wikipedia d'auteur
    """
    try:
        clean_name = author_name.strip()
        
        async with httpx.AsyncClient() as client:
            # 1. Rechercher la page
            search_url = "https://en.wikipedia.org/w/api.php"
            search_params = {
                "action": "query",
                "list": "search",
                "srsearch": clean_name,
                "format": "json",
                "srlimit": 3
            }
            
            search_response = await client.get(search_url, params=search_params, timeout=10)
            
            if search_response.status_code != 200:
                return None
            
            search_data = search_response.json()
            
            # 2. Trouver la page d'auteur
            author_page_title = None
            for result in search_data.get("query", {}).get("search", []):
                title = result.get("title", "")
                snippet = result.get("snippet", "")
                
                if any(keyword in snippet.lower() for keyword in ["author", "writer", "novelist", "poet", "playwright"]):
                    author_page_title = title
                    break
            
            if not author_page_title:
                return None
            
            # 3. RÃ©cupÃ©rer le contenu complet
            content_params = {
                "action": "query",
                "format": "json",
                "prop": "extracts|pageimages",
                "titles": author_page_title,
                "exintro": False,
                "explaintext": True,
                "exsectionformat": "wiki",
                "piprop": "thumbnail",
                "pithumbsize": 300
            }
            
            content_response = await client.get(search_url, params=content_params, timeout=15)
            
            if content_response.status_code == 200:
                content_data = content_response.json()
                
                # 4. RÃ©cupÃ©rer aussi le rÃ©sumÃ© structurÃ©
                summary_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{author_page_title}"
                summary_response = await client.get(summary_url, timeout=10)
                
                summary_data = {}
                if summary_response.status_code == 200:
                    summary_data = summary_response.json()
                
                return {
                    "title": author_page_title,
                    "content": content_data,
                    "summary": summary_data
                }
            
        return None
        
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration du contenu Wikipedia pour {author_name}: {str(e)}")
        return None

def extract_works_from_wikipedia(wikipedia_data: dict, author_name: str) -> List[Dict]:
    """
    Extraire intelligemment les Å“uvres d'un auteur depuis Wikipedia
    """
    works = []
    
    try:
        # RÃ©cupÃ©rer le contenu textuel
        content = ""
        pages = wikipedia_data.get("content", {}).get("query", {}).get("pages", {})
        
        for page_id, page_data in pages.items():
            content = page_data.get("extract", "")
            break
        
        if not content:
            return works
        
        # 1. Patterns pour dÃ©tecter les sÃ©ries principales
        series_patterns = [
            # Patterns spÃ©cifiques par auteur
            (r'Harry Potter(?:\s+(?:series|saga|books|novels))?', "Harry Potter"),
            (r'(?:A\s+)?Song of Ice and Fire|Game of Thrones(?:\s+(?:series|saga))?', "A Song of Ice and Fire"),
            (r'The Lord of the Rings|(?:The\s+)?Hobbit(?:\s+(?:series|saga))?', "The Lord of the Rings"),
            (r'Foundation(?:\s+(?:series|saga|novels))?', "Foundation"),
            (r'Dune(?:\s+(?:series|saga|novels))?', "Dune"),
            (r'Sherlock Holmes(?:\s+(?:series|stories|novels))?', "Sherlock Holmes"),
            (r'Hercule Poirot(?:\s+(?:series|novels|mysteries))?', "Hercule Poirot"),
            (r'Miss Marple(?:\s+(?:series|novels|mysteries))?', "Miss Marple"),
            (r'The Chronicles of Narnia|Narnia(?:\s+(?:series|saga))?', "The Chronicles of Narnia"),
            (r'Discworld(?:\s+(?:series|saga|novels))?', "Discworld"),
            (r'The Wheel of Time(?:\s+(?:series|saga))?', "The Wheel of Time"),
            (r'The Dark Tower(?:\s+(?:series|saga))?', "The Dark Tower"),
            (r'Cormoran Strike(?:\s+(?:series|novels))?', "Cormoran Strike"),
            (r'Fantastic Beasts(?:\s+(?:series|films))?', "Fantastic Beasts"),
            (r'Outlander(?:\s+(?:series|saga|novels))?', "Outlander"),
            (r'The Expanse(?:\s+(?:series|saga|novels))?', "The Expanse"),
            
            # Patterns gÃ©nÃ©riques
            (r'([A-Z][a-zA-Z\s]+)(?:\s+(?:series|saga|cycle|novels?|books?))', None),
            (r'(?:the\s+)?([A-Z][a-zA-Z\s]+?)(?:\s+(?:series|saga|cycle))', None)
        ]
        
        detected_series = set()
        
        for pattern, series_name in series_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            
            for match in matches:
                if series_name:
                    # SÃ©rie prÃ©dÃ©finie
                    final_name = series_name
                else:
                    # SÃ©rie dÃ©tectÃ©e dynamiquement
                    final_name = match if isinstance(match, str) else match[0]
                    final_name = final_name.strip()
                
                # Filtrer les faux positifs
                if (len(final_name) > 3 and len(final_name) < 60 and
                    final_name.lower() not in detected_series and
                    not any(word in final_name.lower() for word in ["wikipedia", "citation", "reference", "source", "page", "edit"])):
                    
                    detected_series.add(final_name.lower())
                    works.append({
                        "title": final_name,
                        "type": "series",
                        "source": "wikipedia",
                        "author": author_name,
                        "confidence": 90 if series_name else 70
                    })
        
        # 2. Patterns pour dÃ©tecter les livres individuels
        individual_patterns = [
            # Livres entre guillemets avec annÃ©es
            r'["""]([^"""]{10,80})["""](?:\s*\((?:19|20)\d{2}\))?',
            # Livres en italique
            r'<i>([^<]{10,80})</i>',
            # Patterns "novel" suivi du titre
            r'(?:novel|book|work|story)\s+["""]([^"""]{10,80})["""]',
            # Patterns "published" suivi du titre
            r'(?:published|wrote|authored)\s+["""]([^"""]{10,80})["""]',
            # Patterns avec annÃ©es
            r'(?:19|20)\d{2}[:\s]+["""]([^"""]{10,80})["""]'
        ]
        
        detected_books = set()
        
        for pattern in individual_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            
            for match in matches:
                title = match.strip()
                title_clean = title.lower()
                
                # Filtrer les faux positifs
                if (len(title) > 10 and len(title) < 80 and
                    title_clean not in detected_books and
                    title_clean not in detected_series and
                    not any(word in title_clean for word in ["wikipedia", "citation", "reference", "source", "page", "edit", "category", "file", "image"])):
                    
                    detected_books.add(title_clean)
                    works.append({
                        "title": title,
                        "type": "individual",
                        "source": "wikipedia",
                        "author": author_name,
                        "confidence": 80
                    })
        
        # 3. Patterns pour dÃ©tecter les annÃ©es de publication
        year_patterns = [
            r'(?:published|wrote|authored)[^.]*?(?:19|20)(\d{2})',
            r'(?:19|20)(\d{2})[^.]*?(?:novel|book|work)',
            r'\((?:19|20)(\d{2})\)'
        ]
        
        # Associer des annÃ©es aux Å“uvres si possible
        for work in works:
            title_context = re.search(rf'{re.escape(work["title"])}.{{0,200}}', content, re.IGNORECASE)
            if title_context:
                context_text = title_context.group(0)
                for year_pattern in year_patterns:
                    year_match = re.search(year_pattern, context_text, re.IGNORECASE)
                    if year_match:
                        work["year"] = int(f"19{year_match.group(1)}" if int(year_match.group(1)) > 50 else f"20{year_match.group(1)}")
                        break
        
        # Trier par confiance et limiter
        works.sort(key=lambda x: x.get("confidence", 0), reverse=True)
        
        return works[:50]  # Limiter Ã  50 Å“uvres maximum
        
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des Å“uvres: {str(e)}")
        return works

def organize_wikipedia_works(works: List[Dict]) -> Dict:
    """
    Organiser les Å“uvres extraites par sÃ©ries et livres individuels
    """
    series_list = []
    individual_books = []
    sources = {"wikipedia": len(works)}
    
    try:
        for work in works:
            if work["type"] == "series":
                series_list.append({
                    "name": work["title"],
                    "type": "series",
                    "author": work["author"],
                    "source": work["source"],
                    "confidence": work.get("confidence", 0),
                    "year": work.get("year"),
                    "books": []  # Sera rempli plus tard si nÃ©cessaire
                })
            else:
                individual_books.append({
                    "title": work["title"],
                    "type": "individual",
                    "author": work["author"],
                    "source": work["source"],
                    "confidence": work.get("confidence", 0),
                    "year": work.get("year")
                })
        
        # Trier par annÃ©e (plus rÃ©cent en premier pour les livres individuels)
        individual_books.sort(key=lambda x: x.get("year", 0) or 0, reverse=True)
        
        # Trier les sÃ©ries par nom
        series_list.sort(key=lambda x: x["name"])
        
        return {
            "series": series_list,
            "individual_books": individual_books,
            "total_books": len(works),
            "total_series": len(series_list),
            "total_individual_books": len(individual_books),
            "sources": sources
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de l'organisation des Å“uvres: {str(e)}")
        return {
            "series": [],
            "individual_books": [],
            "total_books": 0,
            "total_series": 0,
            "total_individual_books": 0,
            "sources": {"wikipedia": 0}
        }

async def search_wikipedia_author(author_name: str) -> Optional[dict]:
    """
    Rechercher un auteur sur Wikipedia et rÃ©cupÃ©rer ses informations
    """
    try:
        # Nettoyer le nom d'auteur pour la recherche
        clean_name = author_name.strip()
        
        # Endpoint REST API de Wikipedia pour rÃ©cupÃ©rer le rÃ©sumÃ©
        wikipedia_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{clean_name}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(wikipedia_url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                # VÃ©rifier que c'est bien un auteur (pas une page d'homonymie)
                if data.get("type") == "standard":
                    return data
                    
            # Si l'API REST Ã©choue, essayer l'API action avec recherche
            search_url = "https://en.wikipedia.org/w/api.php"
            search_params = {
                "action": "query",
                "list": "search",
                "srsearch": clean_name,
                "format": "json",
                "srlimit": 5
            }
            
            search_response = await client.get(search_url, params=search_params, timeout=10)
            
            if search_response.status_code == 200:
                search_data = search_response.json()
                
                # Chercher dans les rÃ©sultats
                for result in search_data.get("query", {}).get("search", []):
                    title = result.get("title", "")
                    snippet = result.get("snippet", "")
                    
                    # VÃ©rifier si c'est un auteur
                    if any(keyword in snippet.lower() for keyword in ["author", "writer", "novelist", "poet", "playwright"]):
                        # RÃ©cupÃ©rer les dÃ©tails de cette page
                        detail_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{title}"
                        detail_response = await client.get(detail_url, timeout=10)
                        
                        if detail_response.status_code == 200:
                            return detail_response.json()
                            
        return None
        
    except Exception as e:
        logger.error(f"Erreur lors de la recherche Wikipedia pour {author_name}: {str(e)}")
        return None

def extract_author_info(wikipedia_data: dict) -> dict:
    """
    Extraire les informations pertinentes d'un auteur depuis les donnÃ©es Wikipedia
    """
    try:
        # Informations de base
        name = wikipedia_data.get("title", "")
        description = wikipedia_data.get("description", "")
        extract = wikipedia_data.get("extract", "")
        
        # URL de l'image
        thumbnail = wikipedia_data.get("thumbnail", {})
        photo_url = thumbnail.get("source", "") if thumbnail else ""
        
        # Biographie courte (limiter Ã  300 caractÃ¨res)
        bio = extract[:300] + "..." if len(extract) > 300 else extract
        
        # Extraire les dates de naissance et dÃ©cÃ¨s depuis la description
        birth_date = ""
        death_date = ""
        
        # Extraire les dates depuis la description (format: "born 1947" ou "1947-2020")
        if description:
            # Pattern pour "born YYYY" ou "YYYY-YYYY"
            born_match = re.search(r'born\s+(\d{4})', description, re.IGNORECASE)
            if born_match:
                birth_date = born_match.group(1)
            
            # Pattern pour date de dÃ©cÃ¨s
            death_match = re.search(r'(\d{4})-(\d{4})', description)
            if death_match:
                birth_date = death_match.group(1)
                death_date = death_match.group(2)
        
        # Extraire des informations plus dÃ©taillÃ©es du texte
        birth_pattern = r'born\s+(?:on\s+)?(\d{1,2}\s+\w+\s+\d{4})|born\s+(?:on\s+)?(\w+\s+\d{1,2},?\s+\d{4})|born\s+(\d{4})'
        death_pattern = r'died\s+(?:on\s+)?(\d{1,2}\s+\w+\s+\d{4})|died\s+(?:on\s+)?(\w+\s+\d{1,2},?\s+\d{4})|died\s+(\d{4})'
        
        birth_match = re.search(birth_pattern, extract, re.IGNORECASE)
        death_match = re.search(death_pattern, extract, re.IGNORECASE)
        
        if birth_match:
            birth_date = birth_match.group(1) or birth_match.group(2) or birth_match.group(3)
        if death_match:
            death_date = death_match.group(1) or death_match.group(2) or death_match.group(3)
        
        # Extraire le comptage des Å“uvres avec patterns amÃ©liorÃ©s
        work_count = 0
        work_summary = ""
        
        # Patterns pour identifier les Å“uvres
        work_patterns = [
            r'author\s+of\s+(\d+)\s+books?',
            r'wrote\s+(\d+)\s+books?',
            r'published\s+(\d+)\s+books?',
            r'(\d+)\s+novels?',
            r'(\d+)\s+published\s+works?',
            r'written\s+(\d+)\s+books?',
            r'(\d+)\s+books?\s+published',
            r'over\s+(\d+)\s+books?',
            r'more\s+than\s+(\d+)\s+books?'
        ]
        
        # Recherche du comptage dans l'extrait
        for pattern in work_patterns:
            match = re.search(pattern, extract, re.IGNORECASE)
            if match:
                try:
                    work_count = int(match.group(1))
                    break
                except (ValueError, IndexError):
                    continue
        
        # Identifier le type d'auteur et crÃ©er un rÃ©sumÃ©
        if "novelist" in extract.lower() or "novels" in extract.lower():
            work_summary = "Romancier"
        elif "poet" in extract.lower() or "poetry" in extract.lower():
            work_summary = "PoÃ¨te"
        elif "playwright" in extract.lower() or "plays" in extract.lower():
            work_summary = "Dramaturge"
        elif "short stories" in extract.lower():
            work_summary = "Auteur de nouvelles"
        elif "fantasy" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur de fantasy"
        elif "horror" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur d'horreur"
        elif "crime" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur de polars"
        elif "children" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur pour enfants"
        elif "author" in extract.lower() or "writer" in extract.lower():
            work_summary = "Ã‰crivain"
        else:
            work_summary = "Auteur"
        
        # Extraire des informations spÃ©ciales depuis l'extrait
        special_info = []
        
        # Rechercher mentions de sÃ©ries cÃ©lÃ¨bres
        if "harry potter" in extract.lower():
            special_info.append("CrÃ©ateur de Harry Potter")
        if "game of thrones" in extract.lower() or "song of ice and fire" in extract.lower():
            special_info.append("CrÃ©ateur de Game of Thrones")
        if "lord of the rings" in extract.lower():
            special_info.append("CrÃ©ateur du Seigneur des Anneaux")
        
        # Rechercher mentions de prix
        if "nobel prize" in extract.lower():
            special_info.append("Prix Nobel de littÃ©rature")
        if "pulitzer" in extract.lower():
            special_info.append("Prix Pulitzer")
        if "hugo award" in extract.lower():
            special_info.append("Prix Hugo")
        
        # AmÃ©liorer le rÃ©sumÃ© avec les informations spÃ©ciales
        if special_info:
            work_summary = f"{work_summary} â€¢ {' â€¢ '.join(special_info)}"
        
        # ComplÃ©ter avec le comptage si disponible
        if work_count > 0:
            work_summary += f" ({work_count} Å“uvres)"
        
        # Extraire l'Å“uvre principale (premiÃ¨re Å“uvre mentionnÃ©e entre guillemets)
        top_work = ""
        title_matches = re.findall(r'["""]([^"""]+)["""]', extract)
        if title_matches:
            # Prendre la premiÃ¨re Å“uvre qui ressemble Ã  un titre
            for title in title_matches:
                if len(title) > 3 and len(title) < 100:  # Filtrer les titres valides
                    top_work = title
                    break
        
        # Alternative: rechercher des titres en italique dans l'HTML
        if not top_work:
            extract_html = wikipedia_data.get("extract_html", "")
            if extract_html:
                italic_matches = re.findall(r'<i>([^<]+)</i>', extract_html)
                if italic_matches:
                    for title in italic_matches:
                        if len(title) > 3 and len(title) < 100:
                            top_work = title
                            break
        
        return {
            "name": name,
            "bio": bio,
            "photo_url": photo_url,
            "birth_date": birth_date,
            "death_date": death_date,
            "work_count": work_count,
            "work_summary": work_summary,
            "top_work": top_work,
            "description": description,
            "wikipedia_url": wikipedia_data.get("content_urls", {}).get("desktop", {}).get("page", "")
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des informations: {str(e)}")
        return {}

@router.get("/author/{author_name}")
async def get_wikipedia_author_info(author_name: str):
    """
    RÃ©cupÃ©rer les informations d'un auteur depuis Wikipedia
    Solution optimale pour donnÃ©es curÃ©es et comptage rÃ©aliste
    """
    try:
        # Rechercher l'auteur sur Wikipedia
        wikipedia_data = await search_wikipedia_author(author_name)
        
        if not wikipedia_data:
            return {
                "found": False,
                "message": f"Auteur '{author_name}' non trouvÃ© sur Wikipedia"
            }
        
        # Extraire les informations structurÃ©es
        author_info = extract_author_info(wikipedia_data)
        
        if not author_info:
            return {
                "found": False,
                "message": f"Impossible d'extraire les informations pour '{author_name}'"
            }
        
        return {
            "found": True,
            "source": "wikipedia",
            "author": author_info
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration des informations pour {author_name}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la rÃ©cupÃ©ration des informations: {str(e)}"
        )

@router.get("/author/{author_name}/works")
async def get_author_works(author_name: str, limit: int = 50):
    """
    RÃ©cupÃ©rer toutes les Å“uvres d'un auteur depuis Wikipedia avec parsing intelligent
    """
    try:
        # 1. RÃ©cupÃ©rer le contenu Wikipedia complet
        wikipedia_data = await get_wikipedia_full_content(author_name)
        
        if not wikipedia_data:
            return {
                "found": False,
                "message": f"Auteur '{author_name}' non trouvÃ© sur Wikipedia"
            }
        
        # 2. Extraire les Å“uvres avec parsing intelligent
        works_data = extract_works_from_wikipedia(wikipedia_data, author_name)
        
        # 3. Organiser les Å“uvres
        organized_works = organize_wikipedia_works(works_data)
        
        return {
            "found": True,
            "author": author_name,
            "series": organized_works["series"],
            "individual_books": organized_works["individual_books"],
            "total_books": organized_works["total_books"],
            "total_series": organized_works["total_series"],
            "total_individual_books": organized_works["total_individual_books"],
            "sources": organized_works["sources"]
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration des Å“uvres pour {author_name}: {str(e)}")
        return {
            "found": False,
            "message": f"Erreur lors de la rÃ©cupÃ©ration des Å“uvres: {str(e)}"
        }

@router.get("/test/{author_name}")
async def test_wikipedia_author(author_name: str):
    """
    Endpoint de test pour vÃ©rifier la rÃ©cupÃ©ration d'informations Wikipedia
    """
    try:
        # Test avec donnÃ©es brutes
        raw_data = await search_wikipedia_author(author_name)
        
        if not raw_data:
            return {"found": False, "message": "Auteur non trouvÃ©"}
        
        # Extraire les informations
        extracted_info = extract_author_info(raw_data)
        
        return {
            "found": True,
            "raw_data": raw_data,
            "extracted_info": extracted_info
        }
        
    except Exception as e:
        return {"error": str(e)}

@router.post("/enrich-series")
async def enrich_series_from_existing_authors(limit: int = 50):
    """
    ðŸš€ Enrichissement automatique des sÃ©ries via Wikipedia
    Analyse les auteurs existants et dÃ©tecte leurs sÃ©ries automatiquement
    """
    try:
        from .enrichment_service import wikipedia_series_service
        
        # Lancement du processus d'enrichissement
        result = await wikipedia_series_service.run_enrichment_process(limit_authors=limit)
        
        if result['success']:
            return {
                "success": True,
                "message": "Enrichissement terminÃ© avec succÃ¨s",
                "stats": {
                    "authors_processed": result['authors_processed'],
                    "total_series_detected": result['total_series_detected'],
                    "high_confidence_series": result['high_confidence_series'],
                    "ultra_harvest_update": result['ultra_harvest_update']
                },
                "completed_at": result['enrichment_completed_at']
            }
        else:
            return {
                "success": False,
                "error": result.get('error', 'Unknown error'),
                "message": "Ã‰chec de l'enrichissement"
            }
            
    except Exception as e:
        logger.error(f"Erreur lors de l'enrichissement des sÃ©ries: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de l'enrichissement: {str(e)}"
        )

@router.get("/enrich-series/status")
async def get_enrichment_status():
    """
    ðŸ“Š Obtenir le statut de l'enrichissement Ultra Harvest
    """
    try:
        from app.database import db
        
        # Compter les sÃ©ries dans Ultra Harvest
        total_series = db.ultra_harvest_wikipedia.count_documents({})
        
        # Statistiques par source
        wikipedia_series = db.ultra_harvest_wikipedia.count_documents({"source": "wikipedia_enrichment"})
        
        # DerniÃ¨re mise Ã  jour
        latest_update = db.ultra_harvest_wikipedia.find_one(
            {"source": "wikipedia_enrichment"},
            sort=[("detected_at", -1)]
        )
        
        return {
            "ultra_harvest_stats": {
                "total_series": total_series,
                "wikipedia_enriched": wikipedia_series,
                "last_update": latest_update.get('detected_at') if latest_update else None
            },
            "enrichment_available": True,
            "message": "Enrichissement Wikipedia disponible"
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration du statut: {str(e)}")
        return {
            "ultra_harvest_stats": {
                "total_series": 0,
                "wikipedia_enriched": 0,
                "last_update": None
            },
            "enrichment_available": False,
            "error": str(e)
        }