"""
Routes pour l'API Wikipedia
RÃ©cupÃ©ration d'informations sur les auteurs avec donnÃ©es curÃ©es
"""

from fastapi import APIRouter, HTTPException
import httpx
import re
from typing import Optional
import logging

router = APIRouter(prefix="/api/wikipedia", tags=["wikipedia"])

logger = logging.getLogger(__name__)

async def search_wikipedia_author(author_name: str) -> Optional[dict]:
    """
    Rechercher un auteur sur Wikipedia et rÃ©cupÃ©rer ses informations
    """
    try:
        # Nettoyer le nom d'auteur pour la recherche
        clean_name = author_name.strip()
        
        # Endpoint REST API de Wikipedia pour rÃ©cupÃ©rer le rÃ©sumÃ©
        wikipedia_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{clean_name}"
        
        async with httpx.AsyncClient() as client:
            response = await client.get(wikipedia_url, timeout=10)
            
            if response.status_code == 200:
                data = response.json()
                
                # VÃ©rifier que c'est bien un auteur (pas une page d'homonymie)
                if data.get("type") == "standard":
                    return data
                    
            # Si l'API REST Ã©choue, essayer l'API action avec recherche
            search_url = "https://en.wikipedia.org/w/api.php"
            search_params = {
                "action": "query",
                "list": "search",
                "srsearch": clean_name,
                "format": "json",
                "srlimit": 5
            }
            
            search_response = await client.get(search_url, params=search_params, timeout=10)
            
            if search_response.status_code == 200:
                search_data = search_response.json()
                
                # Chercher dans les rÃ©sultats
                for result in search_data.get("query", {}).get("search", []):
                    title = result.get("title", "")
                    snippet = result.get("snippet", "")
                    
                    # VÃ©rifier si c'est un auteur
                    if any(keyword in snippet.lower() for keyword in ["author", "writer", "novelist", "poet", "playwright"]):
                        # RÃ©cupÃ©rer les dÃ©tails de cette page
                        detail_url = f"https://en.wikipedia.org/api/rest_v1/page/summary/{title}"
                        detail_response = await client.get(detail_url, timeout=10)
                        
                        if detail_response.status_code == 200:
                            return detail_response.json()
                            
        return None
        
    except Exception as e:
        logger.error(f"Erreur lors de la recherche Wikipedia pour {author_name}: {str(e)}")
        return None

def extract_author_info(wikipedia_data: dict) -> dict:
    """
    Extraire les informations pertinentes d'un auteur depuis les donnÃ©es Wikipedia
    """
    try:
        # Informations de base
        name = wikipedia_data.get("title", "")
        description = wikipedia_data.get("description", "")
        extract = wikipedia_data.get("extract", "")
        
        # URL de l'image
        thumbnail = wikipedia_data.get("thumbnail", {})
        photo_url = thumbnail.get("source", "") if thumbnail else ""
        
        # Biographie courte (limiter Ã  300 caractÃ¨res)
        bio = extract[:300] + "..." if len(extract) > 300 else extract
        
        # Extraire les dates de naissance et dÃ©cÃ¨s depuis la description
        birth_date = ""
        death_date = ""
        
        # Extraire les dates depuis la description (format: "born 1947" ou "1947-2020")
        if description:
            # Pattern pour "born YYYY" ou "YYYY-YYYY"
            born_match = re.search(r'born\s+(\d{4})', description, re.IGNORECASE)
            if born_match:
                birth_date = born_match.group(1)
            
            # Pattern pour date de dÃ©cÃ¨s
            death_match = re.search(r'(\d{4})-(\d{4})', description)
            if death_match:
                birth_date = death_match.group(1)
                death_date = death_match.group(2)
        
        # Extraire des informations plus dÃ©taillÃ©es du texte
        birth_pattern = r'born\s+(?:on\s+)?(\d{1,2}\s+\w+\s+\d{4})|born\s+(?:on\s+)?(\w+\s+\d{1,2},?\s+\d{4})|born\s+(\d{4})'
        death_pattern = r'died\s+(?:on\s+)?(\d{1,2}\s+\w+\s+\d{4})|died\s+(?:on\s+)?(\w+\s+\d{1,2},?\s+\d{4})|died\s+(\d{4})'
        
        birth_match = re.search(birth_pattern, extract, re.IGNORECASE)
        death_match = re.search(death_pattern, extract, re.IGNORECASE)
        
        if birth_match:
            birth_date = birth_match.group(1) or birth_match.group(2) or birth_match.group(3)
        if death_match:
            death_date = death_match.group(1) or death_match.group(2) or death_match.group(3)
        
        # Extraire le comptage des Å“uvres avec patterns amÃ©liorÃ©s
        work_count = 0
        work_summary = ""
        
        # Patterns pour identifier les Å“uvres
        work_patterns = [
            r'author\s+of\s+(\d+)\s+books?',
            r'wrote\s+(\d+)\s+books?',
            r'published\s+(\d+)\s+books?',
            r'(\d+)\s+novels?',
            r'(\d+)\s+published\s+works?',
            r'written\s+(\d+)\s+books?',
            r'(\d+)\s+books?\s+published',
            r'over\s+(\d+)\s+books?',
            r'more\s+than\s+(\d+)\s+books?'
        ]
        
        # Recherche du comptage dans l'extrait
        for pattern in work_patterns:
            match = re.search(pattern, extract, re.IGNORECASE)
            if match:
                try:
                    work_count = int(match.group(1))
                    break
                except (ValueError, IndexError):
                    continue
        
        # Identifier le type d'auteur et crÃ©er un rÃ©sumÃ©
        if "novelist" in extract.lower() or "novels" in extract.lower():
            work_summary = "Romancier"
        elif "poet" in extract.lower() or "poetry" in extract.lower():
            work_summary = "PoÃ¨te"
        elif "playwright" in extract.lower() or "plays" in extract.lower():
            work_summary = "Dramaturge"
        elif "short stories" in extract.lower():
            work_summary = "Auteur de nouvelles"
        elif "fantasy" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur de fantasy"
        elif "horror" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur d'horreur"
        elif "crime" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur de polars"
        elif "children" in extract.lower() and "author" in extract.lower():
            work_summary = "Auteur pour enfants"
        elif "author" in extract.lower() or "writer" in extract.lower():
            work_summary = "Ã‰crivain"
        else:
            work_summary = "Auteur"
        
        # Extraire des informations spÃ©ciales depuis l'extrait
        special_info = []
        
        # Rechercher mentions de sÃ©ries cÃ©lÃ¨bres
        if "harry potter" in extract.lower():
            special_info.append("CrÃ©ateur de Harry Potter")
        if "game of thrones" in extract.lower() or "song of ice and fire" in extract.lower():
            special_info.append("CrÃ©ateur de Game of Thrones")
        if "lord of the rings" in extract.lower():
            special_info.append("CrÃ©ateur du Seigneur des Anneaux")
        
        # Rechercher mentions de prix
        if "nobel prize" in extract.lower():
            special_info.append("Prix Nobel de littÃ©rature")
        if "pulitzer" in extract.lower():
            special_info.append("Prix Pulitzer")
        if "hugo award" in extract.lower():
            special_info.append("Prix Hugo")
        
        # AmÃ©liorer le rÃ©sumÃ© avec les informations spÃ©ciales
        if special_info:
            work_summary = f"{work_summary} â€¢ {' â€¢ '.join(special_info)}"
        
        # ComplÃ©ter avec le comptage si disponible
        if work_count > 0:
            work_summary += f" ({work_count} Å“uvres)"
        
        # Extraire l'Å“uvre principale (premiÃ¨re Å“uvre mentionnÃ©e entre guillemets)
        top_work = ""
        title_matches = re.findall(r'["""]([^"""]+)["""]', extract)
        if title_matches:
            # Prendre la premiÃ¨re Å“uvre qui ressemble Ã  un titre
            for title in title_matches:
                if len(title) > 3 and len(title) < 100:  # Filtrer les titres valides
                    top_work = title
                    break
        
        # Alternative: rechercher des titres en italique dans l'HTML
        if not top_work:
            extract_html = wikipedia_data.get("extract_html", "")
            if extract_html:
                italic_matches = re.findall(r'<i>([^<]+)</i>', extract_html)
                if italic_matches:
                    for title in italic_matches:
                        if len(title) > 3 and len(title) < 100:
                            top_work = title
                            break
        
        return {
            "name": name,
            "bio": bio,
            "photo_url": photo_url,
            "birth_date": birth_date,
            "death_date": death_date,
            "work_count": work_count,
            "work_summary": work_summary,
            "top_work": top_work,
            "description": description,
            "wikipedia_url": wikipedia_data.get("content_urls", {}).get("desktop", {}).get("page", "")
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de l'extraction des informations: {str(e)}")
        return {}

@router.get("/author/{author_name}")
async def get_wikipedia_author_info(author_name: str):
    """
    RÃ©cupÃ©rer les informations d'un auteur depuis Wikipedia
    Solution optimale pour donnÃ©es curÃ©es et comptage rÃ©aliste
    """
    try:
        # Rechercher l'auteur sur Wikipedia
        wikipedia_data = await search_wikipedia_author(author_name)
        
        if not wikipedia_data:
            return {
                "found": False,
                "message": f"Auteur '{author_name}' non trouvÃ© sur Wikipedia"
            }
        
        # Extraire les informations structurÃ©es
        author_info = extract_author_info(wikipedia_data)
        
        if not author_info:
            return {
                "found": False,
                "message": f"Impossible d'extraire les informations pour '{author_name}'"
            }
        
        return {
            "found": True,
            "source": "wikipedia",
            "author": author_info
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration des informations pour {author_name}: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de la rÃ©cupÃ©ration des informations: {str(e)}"
        )

@router.get("/author/{author_name}/works")
async def get_author_works(author_name: str, limit: int = 50):
    """
    RÃ©cupÃ©rer toutes les Å“uvres d'un auteur depuis Wikipedia et OpenLibrary
    """
    try:
        all_works = {
            "series": [],
            "individual_books": [],
            "total_books": 0,
            "sources": {"wikipedia": 0, "openlibrary": 0}
        }
        
        # 1. RÃ©cupÃ©rer les Å“uvres depuis OpenLibrary
        import requests
        
        # Rechercher les livres de l'auteur sur OpenLibrary
        ol_params = {
            "author": author_name,
            "limit": limit,
            "fields": "key,title,author_name,first_publish_year,isbn,cover_i,subject,number_of_pages_median,publisher,series"
        }
        
        ol_response = requests.get("https://openlibrary.org/search.json", params=ol_params, timeout=10)
        
        if ol_response.status_code == 200:
            ol_data = ol_response.json()
            
            # Grouper les livres par sÃ©rie
            series_books = {}
            individual_books = []
            
            for doc in ol_data.get("docs", []):
                book_title = doc.get("title", "")
                series_info = doc.get("series", [])
                
                book_data = {
                    "title": book_title,
                    "year": doc.get("first_publish_year"),
                    "isbn": doc.get("isbn", [""])[0] if doc.get("isbn") else "",
                    "cover_url": f"https://covers.openlibrary.org/b/id/{doc.get('cover_i')}-M.jpg" if doc.get("cover_i") else "",
                    "publisher": ", ".join(doc.get("publisher", [])) if doc.get("publisher") else "",
                    "pages": doc.get("number_of_pages_median"),
                    "source": "openlibrary"
                }
                
                if series_info:
                    # Livre fait partie d'une sÃ©rie
                    series_name = series_info[0] if isinstance(series_info, list) else series_info
                    if series_name not in series_books:
                        series_books[series_name] = {
                            "name": series_name,
                            "books": [],
                            "source": "openlibrary"
                        }
                    series_books[series_name]["books"].append(book_data)
                else:
                    # Livre individuel
                    individual_books.append(book_data)
            
            # Ajouter les sÃ©ries
            for series_name, series_data in series_books.items():
                all_works["series"].append(series_data)
            
            # Ajouter les livres individuels
            all_works["individual_books"].extend(individual_books)
            
            # Compter les livres OpenLibrary
            ol_count = sum(len(s["books"]) for s in series_books.values()) + len(individual_books)
            all_works["sources"]["openlibrary"] = ol_count
            all_works["total_books"] += ol_count
        
        # 2. Enrichir avec des informations Wikipedia si disponible
        wikipedia_data = await search_wikipedia_author(author_name)
        if wikipedia_data:
            extract = wikipedia_data.get("extract", "")
            
            # Rechercher des mentions de sÃ©ries cÃ©lÃ¨bres dans l'extrait
            famous_series = []
            series_patterns = [
                (r'harry potter', "Harry Potter"),
                (r'game of thrones', "Game of Thrones"),
                (r'song of ice and fire', "A Song of Ice and Fire"),
                (r'lord of the rings', "The Lord of the Rings"),
                (r'chronicles of narnia', "The Chronicles of Narnia"),
                (r'wheel of time', "The Wheel of Time"),
                (r'foundation', "Foundation"),
                (r'dune', "Dune"),
                (r'discworld', "Discworld")
            ]
            
            for pattern, series_name in series_patterns:
                if re.search(pattern, extract, re.IGNORECASE):
                    # VÃ©rifier si cette sÃ©rie n'est pas dÃ©jÃ  dans les rÃ©sultats OpenLibrary
                    existing_series = [s["name"] for s in all_works["series"]]
                    if series_name not in existing_series:
                        famous_series.append({
                            "name": series_name,
                            "books": [],
                            "description": f"SÃ©rie cÃ©lÃ¨bre mentionnÃ©e dans Wikipedia",
                            "source": "wikipedia"
                        })
                        all_works["sources"]["wikipedia"] += 1
            
            # Ajouter les sÃ©ries cÃ©lÃ¨bres
            all_works["series"].extend(famous_series)
        
        # 3. Trier les rÃ©sultats
        all_works["series"].sort(key=lambda x: x["name"])
        all_works["individual_books"].sort(key=lambda x: x.get("year", 0) or 0, reverse=True)
        
        return {
            "found": True,
            "author": author_name,
            "works": all_works
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration des Å“uvres pour {author_name}: {str(e)}")
        return {
            "found": False,
            "message": f"Erreur lors de la rÃ©cupÃ©ration des Å“uvres: {str(e)}"
        }

@router.get("/test/{author_name}")
async def test_wikipedia_author(author_name: str):
    """
    Endpoint de test pour vÃ©rifier la rÃ©cupÃ©ration d'informations Wikipedia
    """
    try:
        # Test avec donnÃ©es brutes
        raw_data = await search_wikipedia_author(author_name)
        
        if not raw_data:
            return {"found": False, "message": "Auteur non trouvÃ©"}
        
        # Extraire les informations
        extracted_info = extract_author_info(raw_data)
        
        return {
            "found": True,
            "raw_data": raw_data,
            "extracted_info": extracted_info
        }
        
    except Exception as e:
        return {"error": str(e)}

@router.post("/enrich-series")
async def enrich_series_from_existing_authors(limit: int = 50):
    """
    ðŸš€ Enrichissement automatique des sÃ©ries via Wikipedia
    Analyse les auteurs existants et dÃ©tecte leurs sÃ©ries automatiquement
    """
    try:
        from .enrichment_service import wikipedia_series_service
        
        # Lancement du processus d'enrichissement
        result = await wikipedia_series_service.run_enrichment_process(limit_authors=limit)
        
        if result['success']:
            return {
                "success": True,
                "message": "Enrichissement terminÃ© avec succÃ¨s",
                "stats": {
                    "authors_processed": result['authors_processed'],
                    "total_series_detected": result['total_series_detected'],
                    "high_confidence_series": result['high_confidence_series'],
                    "ultra_harvest_update": result['ultra_harvest_update']
                },
                "completed_at": result['enrichment_completed_at']
            }
        else:
            return {
                "success": False,
                "error": result.get('error', 'Unknown error'),
                "message": "Ã‰chec de l'enrichissement"
            }
            
    except Exception as e:
        logger.error(f"Erreur lors de l'enrichissement des sÃ©ries: {str(e)}")
        raise HTTPException(
            status_code=500,
            detail=f"Erreur lors de l'enrichissement: {str(e)}"
        )

@router.get("/enrich-series/status")
async def get_enrichment_status():
    """
    ðŸ“Š Obtenir le statut de l'enrichissement Ultra Harvest
    """
    try:
        from app.database import db
        
        # Compter les sÃ©ries dans Ultra Harvest
        total_series = db.ultra_harvest_wikipedia.count_documents({})
        
        # Statistiques par source
        wikipedia_series = db.ultra_harvest_wikipedia.count_documents({"source": "wikipedia_enrichment"})
        
        # DerniÃ¨re mise Ã  jour
        latest_update = db.ultra_harvest_wikipedia.find_one(
            {"source": "wikipedia_enrichment"},
            sort=[("detected_at", -1)]
        )
        
        return {
            "ultra_harvest_stats": {
                "total_series": total_series,
                "wikipedia_enriched": wikipedia_series,
                "last_update": latest_update.get('detected_at') if latest_update else None
            },
            "enrichment_available": True,
            "message": "Enrichissement Wikipedia disponible"
        }
        
    except Exception as e:
        logger.error(f"Erreur lors de la rÃ©cupÃ©ration du statut: {str(e)}")
        return {
            "ultra_harvest_stats": {
                "total_series": 0,
                "wikipedia_enriched": 0,
                "last_update": None
            },
            "enrichment_available": False,
            "error": str(e)
        }