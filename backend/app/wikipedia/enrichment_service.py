"""
ðŸš€ Wikipedia Series Enrichment Service
Service d'enrichissement automatique des sÃ©ries via Wikipedia
"""

import asyncio
import aiohttp
import re
from datetime import datetime
from typing import List, Dict, Optional
from app.database import db
import logging

logger = logging.getLogger(__name__)

class WikipediaSeriesService:
    """Service d'enrichissement automatique des sÃ©ries"""
    
    def __init__(self):
        self.base_url = "https://en.wikipedia.org/api/rest_v1/page/summary/"
        self.known_series = {
            'Harry Potter', 'The Dark Tower', 'Discworld', 'Wheel of Time',
            'A Song of Ice and Fire', 'The Hunger Games', 'Percy Jackson',
            'The Mortal Instruments', 'Dune', 'Foundation', 'Earthsea',
            'The Chronicles of Narnia', 'His Dark Materials', 'The Dresden Files',
            'The Witcher', 'Mistborn', 'The Stormlight Archive', 'The Expanse',
            'The Southern Reach', 'The Kingkiller Chronicle', 'The First Law',
            'The Malazan Book of the Fallen', 'The Gentleman Bastard',
            'The Broken Empire', 'The Farseer Trilogy', 'The Culture',
            'Hyperion Cantos', 'The Imperial Radch', 'The Wayfarers',
            'The Expanse', 'The Locked Tomb', 'The Poppy War'
        }
    
    async def get_author_wikipedia_page(self, session: aiohttp.ClientSession, author_name: str) -> Optional[Dict]:
        """RÃ©cupÃ¨re la page Wikipedia d'un auteur"""
        try:
            formatted_name = author_name.replace(' ', '_')
            url = f"{self.base_url}{formatted_name}"
            
            async with session.get(url, timeout=aiohttp.ClientTimeout(total=10)) as response:
                if response.status == 200:
                    data = await response.json()
                    logger.info(f"âœ… Page Wikipedia trouvÃ©e : {author_name}")
                    return data
                else:
                    logger.warning(f"âŒ Pas de page Wikipedia : {author_name} ({response.status})")
                    return None
                    
        except Exception as e:
            logger.error(f"âŒ Erreur Wikipedia {author_name}: {str(e)}")
            return None
    
    def extract_series_from_wikipedia(self, description: str, author_name: str) -> List[Dict]:
        """Extrait les sÃ©ries depuis la description Wikipedia"""
        series_found = []
        
        # Patterns spÃ©cialisÃ©s pour diffÃ©rents types de sÃ©ries
        improved_patterns = [
            # SÃ©ries explicites
            (r'(?:author|creator|writer) of (?:the )?([A-Z][a-zA-Z\s]+?) (?:series|saga|cycle)', 90),
            (r'(?:wrote|created|best known for) (?:the )?([A-Z][a-zA-Z\s]+?) (?:series|novels|books)', 85),
            (r'(?:the )?([A-Z][a-zA-Z\s]+?) (?:fantasy|science fiction|mystery|horror) series', 85),
            
            # SÃ©ries connues spÃ©cifiques
            (r'(Harry Potter)', 95),
            (r'(The Dark Tower)', 95),
            (r'(Discworld)', 95),
            (r'(Wheel of Time)', 95),
            (r'(A Song of Ice and Fire)', 95),
            (r'(The Hunger Games)', 95),
            (r'(Percy Jackson)', 95),
            (r'(The Mortal Instruments)', 95),
            (r'(Foundation)', 95),
            (r'(Dune)', 95),
            (r'(Earthsea)', 95),
            (r'(The Dresden Files)', 95),
            (r'(Mistborn)', 95),
            (r'(The Stormlight Archive)', 95),
            
            # Patterns pour volumes multiples
            (r'([A-Z][a-zA-Z\s]+?) (?:trilogy|tetralogy|pentalogy)', 80),
            (r'([A-Z][a-zA-Z\s]+?) (?:book series|novel series)', 80),
        ]
        
        for pattern, confidence in improved_patterns:
            matches = re.findall(pattern, description, re.IGNORECASE)
            for match in matches:
                series_name = match.strip()
                
                if self.is_valid_series_name(series_name):
                    series_info = {
                        'name': series_name,
                        'author': author_name,
                        'source': 'wikipedia_api',
                        'detected_at': datetime.now().isoformat(),
                        'confidence': confidence,
                        'validation_passed': True
                    }
                    
                    # Ã‰viter les doublons
                    if not any(s['name'].lower() == series_name.lower() for s in series_found):
                        series_found.append(series_info)
                        logger.info(f"ðŸ“š SÃ©rie dÃ©tectÃ©e : {series_name} par {author_name} ({confidence}%)")
        
        return series_found
    
    def is_valid_series_name(self, name: str) -> bool:
        """Valide si le nom est une vraie sÃ©rie"""
        invalid_keywords = [
            'literature', 'fiction', 'books', 'novels', 'works', 'writing',
            'style', 'genre', 'career', 'life', 'biography', 'awards',
            'university', 'college', 'school', 'education', 'degree',
            'her', 'his', 'and', 'the author', 'bestselling', 'award',
            'children', 'young adult', 'dystopian', 'horror', 'fantasy',
            'science fiction', 'mystery', 'graphic', 'unfinished'
        ]
        
        name_lower = name.lower().strip()
        
        # VÃ©rifications de base
        if len(name) < 3 or len(name) > 60:
            return False
            
        # Filtrage des mots-clÃ©s invalides
        if any(keyword in name_lower for keyword in invalid_keywords):
            return False
            
        # Doit contenir des lettres
        if not re.search(r'[a-zA-Z]', name):
            return False
            
        # Filtrage des fragments
        if name_lower.startswith(('a ', 'an ', 'the ', 'and ', 'her ', 'his ')):
            return False
            
        # VÃ©rification sÃ©ries connues
        if any(known.lower() in name_lower for known in self.known_series):
            return True
            
        # Doit ressembler Ã  un titre
        if re.match(r'^[A-Z][a-zA-Z\s]+$', name):
            return True
            
        return False
    
    def get_existing_authors_from_db(self) -> List[str]:
        """RÃ©cupÃ¨re les auteurs existants depuis la base de donnÃ©es"""
        try:
            # AgrÃ©gation pour obtenir les auteurs uniques
            pipeline = [
                {"$group": {"_id": "$author", "count": {"$sum": 1}}},
                {"$match": {"count": {"$gte": 1}}},  # Au moins 1 livre
                {"$sort": {"count": -1}},  # Tri par nombre de livres
                {"$limit": 50}  # Limite Ã  50 auteurs
            ]
            
            authors_cursor = db.books.aggregate(pipeline)
            authors = [doc["_id"] for doc in authors_cursor if doc["_id"]]
            
            logger.info(f"ðŸ“š {len(authors)} auteurs trouvÃ©s dans la base")
            return authors
            
        except Exception as e:
            logger.error(f"âŒ Erreur rÃ©cupÃ©ration auteurs : {str(e)}")
            return []
    
    async def enrich_series_for_author(self, session: aiohttp.ClientSession, author_name: str) -> List[Dict]:
        """Enrichit les sÃ©ries pour un auteur"""
        logger.info(f"ðŸš€ Enrichissement pour : {author_name}")
        
        # RÃ©cupÃ©ration page Wikipedia
        wiki_page = await self.get_author_wikipedia_page(session, author_name)
        if not wiki_page:
            return []
        
        # Extraction des sÃ©ries
        description = wiki_page.get('extract', '')
        if not description:
            logger.warning(f"âŒ Pas de description pour : {author_name}")
            return []
        
        series_found = self.extract_series_from_wikipedia(description, author_name)
        logger.info(f"âœ… {len(series_found)} sÃ©ries trouvÃ©es pour {author_name}")
        
        return series_found
    
    def save_series_to_ultra_harvest(self, series_data: List[Dict]) -> Dict:
        """Sauvegarde les sÃ©ries dans Ultra Harvest"""
        try:
            # Collection Ultra Harvest (on peut crÃ©er une collection dÃ©diÃ©e)
            ultra_harvest_collection = db.ultra_harvest_wikipedia
            
            # Statistiques
            new_series = 0
            updated_series = 0
            
            for series in series_data:
                # PrÃ©paration document
                document = {
                    'name': series['name'],
                    'author': series['author'],
                    'source': 'wikipedia_enrichment',
                    'confidence': series['confidence'],
                    'detected_at': series['detected_at'],
                    'category': 'auto_detected',  # CatÃ©gorie par dÃ©faut
                    'status': 'active',
                    'enrichment_version': '2.0'
                }
                
                # Upsert (insert ou update)
                result = ultra_harvest_collection.replace_one(
                    {
                        'name': series['name'],
                        'author': series['author']
                    },
                    document,
                    upsert=True
                )
                
                if result.upserted_id:
                    new_series += 1
                else:
                    updated_series += 1
            
            logger.info(f"ðŸ’¾ Sauvegarde : {new_series} nouvelles, {updated_series} mises Ã  jour")
            
            return {
                'new_series': new_series,
                'updated_series': updated_series,
                'total_processed': len(series_data)
            }
            
        except Exception as e:
            logger.error(f"âŒ Erreur sauvegarde Ultra Harvest : {str(e)}")
            return {
                'new_series': 0,
                'updated_series': 0,
                'total_processed': 0,
                'error': str(e)
            }
    
    async def run_enrichment_process(self, limit_authors: int = 50) -> Dict:
        """Lance le processus d'enrichissement complet"""
        logger.info(f"ðŸš€ DÃ©marrage enrichissement automatique (limite: {limit_authors})")
        
        # RÃ©cupÃ©ration des auteurs existants
        existing_authors = self.get_existing_authors_from_db()
        if not existing_authors:
            logger.error("âŒ Aucun auteur trouvÃ© dans la base")
            return {
                'success': False,
                'error': 'No authors found in database'
            }
        
        # Limitation
        authors_to_process = existing_authors[:limit_authors]
        logger.info(f"ðŸ“š Traitement de {len(authors_to_process)} auteurs")
        
        # Enrichissement parallÃ¨le
        all_series = []
        
        async with aiohttp.ClientSession() as session:
            # Traitement avec semaphore pour limiter les requÃªtes
            semaphore = asyncio.Semaphore(3)  # Max 3 requÃªtes simultanÃ©es
            
            async def process_author_with_semaphore(author):
                async with semaphore:
                    return await self.enrich_series_for_author(session, author)
            
            # ExÃ©cution parallÃ¨le
            results = await asyncio.gather(
                *[process_author_with_semaphore(author) for author in authors_to_process],
                return_exceptions=True
            )
            
            # AgrÃ©gation des rÃ©sultats
            for result in results:
                if isinstance(result, list):
                    all_series.extend(result)
        
        # Filtrage par confiance
        high_confidence_series = [s for s in all_series if s['confidence'] >= 85]
        
        # Sauvegarde dans Ultra Harvest
        save_result = self.save_series_to_ultra_harvest(high_confidence_series)
        
        # Statistiques finales
        final_stats = {
            'success': True,
            'authors_processed': len(authors_to_process),
            'total_series_detected': len(all_series),
            'high_confidence_series': len(high_confidence_series),
            'ultra_harvest_update': save_result,
            'enrichment_completed_at': datetime.now().isoformat(),
            'version': '2.0'
        }
        
        logger.info(f"ðŸŽ¯ Enrichissement terminÃ© : {len(high_confidence_series)} sÃ©ries haute confiance")
        
        return final_stats

# Instance globale du service
wikipedia_series_service = WikipediaSeriesService()