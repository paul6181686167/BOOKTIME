#!/usr/bin/env python3
"""
üöÄ ULTRA HARVEST MEGA EXPANSION - CONFIANCE 70%
Script ultra-sp√©cialis√© pour d√©couvrir des s√©ries dans des niches sp√©cifiques
"""

import asyncio
import aiohttp
import json
import random
import time
from datetime import datetime
from pathlib import Path
import logging
import re

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger('MegaExpansion')

class MegaExpansionHarvest:
    """Harvest ultra-sp√©cialis√© pour niches non explor√©es"""
    
    def __init__(self):
        self.new_series_found = []
        self.books_analyzed = 0
        self.start_time = datetime.now()
        
        # Charger s√©ries existantes
        self.existing_series = set()
        self._load_existing_series()
    
    def _load_existing_series(self):
        """Charger s√©ries existantes"""
        series_path = Path('/app/backend/data/extended_series_database.json')
        if series_path.exists():
            with open(series_path, 'r') as f:
                series_data = json.load(f)
                for series in series_data:
                    name = series.get('name', '').lower()
                    if name:
                        self.existing_series.add(name)
        logger.info(f"‚úÖ {len(self.existing_series)} s√©ries existantes charg√©es")
    
    def _generate_ultra_specialized_queries(self) -> list:
        """G√©n√©ration de requ√™tes ultra-sp√©cialis√©es pour niches"""
        
        specialized_queries = [
            # Light novels japonais
            'subject:"light novel" AND (volume OR vol)',
            'title:"light novel" AND series',
            '"„É©„Ç§„Éà„Éé„Éô„É´" AND volume',
            
            # Web novels et adaptations
            'subject:"web novel" AND adaptation',
            'subject:"webnovel" AND (series OR volume)',
            '"web novel" AND published',
            
            # Manhwa cor√©ens
            'language:kor AND (volume OR series)',
            'subject:"manhwa" AND (vol OR tome)',
            'subject:"korean comic" AND series',
            
            # Manhua chinois
            'language:chi AND (volume OR series)',
            'subject:"manhua" AND (vol OR tome)',
            'subject:"chinese comic" AND series',
            
            # Romans graphiques ind√©pendants
            'subject:"indie comic" AND (volume OR series)',
            'subject:"independent comic" AND series',
            'publisher:"Image Comics" AND (limited OR ongoing)',
            
            # Fantasy urbaine moderne
            'subject:"urban fantasy" AND (series OR saga)',
            'subject:"paranormal romance" AND (book OR volume)',
            'subject:"supernatural" AND (series OR collection)',
            
            # LitRPG et GameLit
            'subject:"litrpg" AND (book OR volume)',
            'subject:"gamelit" AND series',
            '"virtual reality" AND novel AND series',
            
            # Cultivation novels
            'subject:"cultivation novel" AND (book OR volume)',
            'subject:"xianxia" AND novel',
            'subject:"wuxia" AND series',
            
            # Science-fiction moderne
            'subject:"cyberpunk" AND (series OR trilogy)',
            'subject:"space opera" AND (book OR volume)',
            'subject:"hard science fiction" AND series',
            
            # Young Adult sp√©cialis√©
            'subject:"ya fantasy" AND (series OR saga)',
            'subject:"teen fiction" AND (book OR series)',
            'subject:"coming of age" AND series',
            
            # Genres √©mergents
            'subject:"climate fiction" AND series',
            'subject:"solarpunk" AND novel',
            'subject:"biopunk" AND series',
            
            # Auto-√©dition prolifique
            'publisher:"CreateSpace" AND (series OR saga)',
            'publisher:"Kindle Direct Publishing" AND series',
            'subject:"self published" AND (book OR volume)',
            
            # Comics alternatifs
            'publisher:"Vertigo" AND (series OR collection)',
            'publisher:"Dark Horse" AND (ongoing OR limited)',
            'subject:"alternative comics" AND series',
            
            # Adaptations multi-m√©dias
            'subject:"anime adaptation" AND novel',
            'subject:"manga adaptation" AND series',
            'subject:"video game" AND novel AND series',
            
            # Litt√©rature genre sp√©cialis√©e
            'subject:"sword and sorcery" AND (series OR saga)',
            'subject:"steampunk" AND (novel OR series)',
            'subject:"alternate history" AND series',
            
            # Non-fiction s√©rialis√©e
            'subject:"true crime" AND (series OR collection)',
            'subject:"biography" AND (series OR volume)',
            'subject:"memoir" AND (series OR part)',
            
            # √âditeurs sp√©cialis√©s manga
            'publisher:"Viz Media" AND (vol OR volume)',
            'publisher:"Kodansha" AND series',
            'publisher:"Shogakukan" AND volume',
            
            # Collectibles et √©ditions sp√©ciales
            '"limited edition" AND (series OR collection)',
            '"collector edition" AND (volume OR set)',
            '"omnibus edition" AND series',
            
            # S√©ries audio
            'subject:"audiobook" AND (series OR collection)',
            'subject:"podcast" AND series',
            'subject:"radio drama" AND series',
            
            # Formats √©mergents
            'subject:"webtoon" AND series',
            'subject:"webcomic" AND (ongoing OR collection)',
            'subject:"digital comic" AND series',
            
            # Th√©matiques sp√©cialis√©es
            'subject:"medical thriller" AND series',
            'subject:"legal thriller" AND (book OR series)',
            'subject:"military science fiction" AND series',
            
            # Langues moins communes
            'language:rus AND (—Ç–æ–º OR —Å–µ—Ä–∏—è)',  # Russe
            'language:jpn AND (Â∑ª OR „Ç∑„É™„Éº„Ç∫)',   # Japonais
            'language:fra AND (tome OR s√©rie)',   # Fran√ßais
            'language:deu AND (band OR serie)',   # Allemand
            
            # Auteurs prolifiques sp√©cialis√©s
            'author:"Brandon Sanderson" AND (book OR volume)',
            'author:"Rick Riordan" AND series',
            'author:"James S.A. Corey" AND book',
            'author:"Cassandra Clare" AND series',
            
            # Franchises transm√©dias
            '"Warhammer 40000" AND novel',
            '"Dungeons and Dragons" AND novel',
            '"Magic: The Gathering" AND book',
            '"World of Warcraft" AND novel',
            
            # Sub-genres fantasy
            'subject:"high fantasy" AND (epic OR saga)',
            'subject:"dark fantasy" AND series',
            'subject:"epic fantasy" AND (book OR volume)',
            
            # Romance sp√©cialis√©e
            'subject:"historical romance" AND series',
            'subject:"contemporary romance" AND (book OR series)',
            'subject:"paranormal romance" AND saga',
            
            # Litt√©rature jeunesse avanc√©e
            'subject:"middle grade" AND (series OR adventure)',
            'subject:"chapter book" AND series',
            'subject:"early reader" AND collection'
        ]
        
        # M√©langer pour diversit√©
        random.shuffle(specialized_queries)
        return specialized_queries
    
    async def search_openlibrary_advanced(self, query: str, limit: int = 200) -> list:
        """Recherche avanc√©e Open Library avec timeout √©tendu"""
        
        url = "https://openlibrary.org/search.json"
        params = {
            'q': query,
            'limit': limit,
            'fields': 'key,title,author_name,first_publish_year,subject,publisher,language,isbn'
        }
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url, params=params, timeout=45) as response:
                    if response.status == 200:
                        data = await response.json()
                        books = data.get('docs', [])
                        logger.info(f"üîç Requ√™te '{query[:40]}...' ‚Üí {len(books)} livres")
                        return books
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Erreur recherche '{query[:30]}...': {e}")
        
        return []
    
    def detect_ultra_series_patterns(self, books: list, query_type: str) -> list:
        """D√©tection ultra-sophistiqu√©e avec confiance 70%"""
        
        series_candidates = {}
        
        for book in books:
            title = book.get('title', '')
            authors = book.get('author_name', [])
            author = authors[0] if authors else 'Unknown'
            subjects = book.get('subject', [])
            
            # Patterns ultra-sophistiqu√©s
            advanced_patterns = [
                # Patterns classiques am√©lior√©s
                r'(.+?)\s+(?:volume|vol\.?|tome|book|part|#|chapter|episode)\s*(\d+)',
                r'(.+?)\s+(\d+)(?:st|nd|rd|th)?\s*(?:edition|volume|book|part)?',
                r'(.+?)\s*:\s*(?:book|volume|part|chapter)\s*(\d+)',
                r'(.+?)\s*\((?:book|vol\.?|volume|part)\s*(\d+)\)',
                
                # Patterns sp√©cialis√©s light novels
                r'(.+?)\s+(?:light novel|ln)\s*(\d+)',
                r'(.+?)\s*„É©„Ç§„Éà„Éé„Éô„É´\s*(\d+)',
                
                # Patterns manga/manhwa/manhua
                r'(.+?)\s*,?\s*(?:vol\.?|volume|tome)\s*(\d+)',
                r'(.+?)\s*#(\d+)',
                r'(.+?)\s*(\d+)(?:\s*of\s*\d+)?',
                r'(.+?)\s*Â∑ª\s*(\d+)',  # Japonais
                r'(.+?)\s*Í∂å\s*(\d+)',   # Cor√©en
                
                # Patterns s√©ries sp√©cialis√©es
                r'(.+?)\s+(?:saga|series|collection)\s*(\d+)',
                r'(.+?)\s*-\s*(?:book|volume|part)\s*(\d+)',
                r'(.+?)\s+(?:season|arc)\s*(\d+)',
                
                # Patterns formats sp√©ciaux
                r'(.+?)\s+(?:omnibus|deluxe|complete)\s*(\d+)',
                r'(.+?)\s*:\s*(?:season|series)\s*(\d+)',
                
                # Patterns romans web
                r'(.+?)\s+(?:novel|ln)\s*(\d+)',
                r'(.+?)\s+web\s*novel\s*(\d+)',
                
                # Patterns romans graphiques
                r'(.+?)\s+(?:graphic novel|gn)\s*(\d+)',
                r'(.+?)\s*graphic\s+novel\s*(\d+)'
            ]
            
            for pattern in advanced_patterns:
                match = re.search(pattern, title, re.IGNORECASE)
                if match:
                    series_name = match.group(1).strip()
                    try:
                        volume_num = int(match.group(2))
                    except (ValueError, IndexError):
                        continue
                    
                    # Nettoyage nom s√©rie
                    series_name = re.sub(r'\s*[,.:;-]\s*$', '', series_name)
                    series_name = series_name.strip()
                    
                    # Crit√®res de qualit√© stricts
                    if (len(series_name) < 2 or 
                        volume_num > 200 or 
                        volume_num < 1 or
                        series_name.lower() in self.existing_series):
                        continue
                    
                    # Cr√©er candidat
                    key = (series_name, author)
                    if key not in series_candidates:
                        series_candidates[key] = {
                            'books': [],
                            'confidence_scores': [],
                            'volumes': set(),
                            'subjects': set(),
                            'query_type': query_type
                        }
                    
                    # Calculer score de confiance sophistiqu√©
                    confidence = 75  # Base √©lev√©e
                    
                    # Bonus contexte de requ√™te
                    if 'light novel' in query_type.lower():
                        confidence += 10
                    if 'manga' in query_type.lower() or 'comic' in query_type.lower():
                        confidence += 8
                    if 'series' in title.lower():
                        confidence += 5
                    
                    # Bonus patterns avanc√©s
                    if 'vol' in title.lower() or 'volume' in title.lower():
                        confidence += 7
                    if volume_num > 1:
                        confidence += 5
                    
                    # Bonus sujets pertinents
                    relevant_subjects = ['fiction', 'fantasy', 'comics', 'manga', 'novel']
                    if any(subj in ' '.join(subjects).lower() for subj in relevant_subjects):
                        confidence += 8
                    
                    series_candidates[key]['books'].append(book)
                    series_candidates[key]['confidence_scores'].append(confidence)
                    series_candidates[key]['volumes'].add(volume_num)
                    series_candidates[key]['subjects'].update(subjects[:5])  # Limiter
                    break
        
        # Validation avec seuil 70%
        valid_series = []
        for (series_name, author), data in series_candidates.items():
            max_confidence = max(data['confidence_scores']) if data['confidence_scores'] else 0
            unique_volumes = len(data['volumes'])
            
            # SEUIL 70% + au moins 2 volumes OU confiance tr√®s √©lev√©e
            if (max_confidence >= 70 and unique_volumes >= 2) or max_confidence >= 90:
                category = self._detect_advanced_category(data['books'], data['subjects'], data['query_type'])
                
                series_entry = {
                    'name': series_name,
                    'authors': [author],
                    'category': category,
                    'volumes': unique_volumes,
                    'confidence_score': max_confidence,
                    'source': f'mega_expansion_70_{data["query_type"][:20]}',
                    'detection_date': datetime.now().isoformat(),
                    'books_found': len(data['books']),
                    'subjects': list(data['subjects'])[:5],
                    'query_context': data['query_type']
                }
                valid_series.append(series_entry)
                self.new_series_found.append(series_entry)
                
                logger.info(f"üéØ S√©rie: {series_name} ({category}) - {max_confidence}% - {unique_volumes} vols")
        
        return valid_series
    
    def _detect_advanced_category(self, books: list, subjects: set, query_type: str) -> str:
        """D√©tection cat√©gorie avanc√©e"""
        
        subjects_str = ' '.join(subjects).lower() + ' ' + query_type.lower()
        
        # Analyse des titres aussi
        titles_str = ' '.join([book.get('title', '') for book in books]).lower()
        combined = subjects_str + ' ' + titles_str
        
        # Manga/Anime
        manga_terms = ['manga', 'anime', 'japanese', 'shonen', 'seinen', 'shoujo', 'light novel', 'ln', 'manhwa', 'manhua']
        if any(term in combined for term in manga_terms):
            return 'manga'
        
        # BD/Comics
        comic_terms = ['comic', 'graphic', 'superhero', 'marvel', 'dc', 'vertigo', 'image', 'dark horse']
        if any(term in combined for term in comic_terms):
            return 'bd'
        
        # Par d√©faut roman
        return 'roman'
    
    async def run_mega_expansion(self, max_queries: int = 80):
        """Ex√©cution mega expansion"""
        
        logger.info(f"""
üöÄ MEGA EXPANSION HARVEST - CONFIANCE 70%
=========================================
üéØ Requ√™tes sp√©cialis√©es: {max_queries}
üìä Seuil confiance: 70%
üîç Focus: Niches ultra-sp√©cialis√©es
üóÑÔ∏è Base actuelle: {len(self.existing_series)} s√©ries
=========================================
""")
        
        queries = self._generate_ultra_specialized_queries()
        successful_queries = 0
        
        for i, query in enumerate(queries[:max_queries]):
            logger.info(f"üîç [{i+1}/{max_queries}] {query}")
            
            # Recherche
            books = await self.search_openlibrary_advanced(query, limit=200)
            self.books_analyzed += len(books)
            
            # D√©tection
            if books:
                series_found = self.detect_ultra_series_patterns(books, query)
                if series_found:
                    successful_queries += 1
                    logger.info(f"‚úÖ {len(series_found)} s√©ries trouv√©es!")
            
            # D√©lai respectueux
            await asyncio.sleep(0.3)
            
            # Checkpoint tous les 20
            if (i + 1) % 20 == 0:
                elapsed = (datetime.now() - self.start_time).total_seconds()
                logger.info(f"üìä Checkpoint: {len(self.new_series_found)} s√©ries trouv√©es en {elapsed:.1f}s")
        
        return await self._save_mega_results()
    
    async def _save_mega_results(self):
        """Sauvegarde r√©sultats mega expansion"""
        
        if not self.new_series_found:
            logger.info("‚ÑπÔ∏è Aucune nouvelle s√©rie d√©tect√©e")
            return {'success': True, 'new_series': 0}
        
        # Sauvegarde
        series_path = Path('/app/backend/data/extended_series_database.json')
        
        # Backup
        backup_path = Path(f'/app/backups/series_detection/backup_mega_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json')
        backup_path.parent.mkdir(parents=True, exist_ok=True)
        
        # Charger et fusionner
        if series_path.exists():
            with open(series_path, 'r') as f:
                existing_data = json.load(f)
        else:
            existing_data = []
        
        # Backup s√©curis√©
        with open(backup_path, 'w') as f:
            json.dump(existing_data + self.new_series_found, f, indent=2, ensure_ascii=False)
        
        # Ajout nouvelles s√©ries
        existing_data.extend(self.new_series_found)
        
        # Sauvegarde finale
        with open(series_path, 'w') as f:
            json.dump(existing_data, f, indent=2, ensure_ascii=False)
        
        elapsed = (datetime.now() - self.start_time).total_seconds()
        
        result = {
            'success': True,
            'new_series': len(self.new_series_found),
            'total_series_now': len(existing_data),
            'books_analyzed': self.books_analyzed,
            'execution_time': f"{elapsed:.1f}s",
            'expansion_rate': len(self.new_series_found) / (elapsed/60) if elapsed > 0 else 0
        }
        
        logger.info(f"""
‚úÖ MEGA EXPANSION TERMIN√âE
=========================
üéØ Nouvelles s√©ries: {result['new_series']}
üìö Total maintenant: {result['total_series_now']}
üìñ Livres analys√©s: {result['books_analyzed']}
‚è±Ô∏è Dur√©e: {result['execution_time']}
üìà Taux: {result['expansion_rate']:.1f} s√©ries/min
=========================
""")
        
        return result

async def main():
    """Point d'entr√©e mega expansion"""
    
    harvester = MegaExpansionHarvest()
    result = await harvester.run_mega_expansion(max_queries=100)
    
    if result['success'] and result['new_series'] > 0:
        print(f"\nüéâ MEGA EXPANSION R√âUSSIE! {result['new_series']} nouvelles s√©ries!")
        print(f"üìä Total s√©ries: {result['total_series_now']}")
        print(f"‚ö° Taux d'expansion: {result['expansion_rate']:.1f} s√©ries/min")
    else:
        print(f"\nüí° Mega expansion termin√©e. Total: {result.get('total_series_now', 'N/A')} s√©ries")

if __name__ == "__main__":
    asyncio.run(main())